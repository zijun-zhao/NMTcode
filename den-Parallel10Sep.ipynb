{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/penglab/anaconda3/lib/python3.7/site-packages/neuro_morpho_toolbox/\n",
      "Loading CCF Atlas data...\n",
      "Loading time: 0.90\n",
      "Loading CCF brain structure data...\n",
      "Loading time: 0.01\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "import neuro_morpho_toolbox as nmt\n",
    "%matplotlib inline\n",
    "#ns=nmt.neuron_set('/home/penglab/Documents/Janelia_1000')\n",
    "import pickle\n",
    "pickle_in = open(\"/home/penglab/FeaCal/ns.pickle\",\"rb\")\n",
    "example_ = pickle.load(pickle_in)\n",
    "ns= example_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDuplicateColumns(df):\n",
    "    '''\n",
    "    Get a list of duplicate columns.\n",
    "    It will iterate over all the columns in dataframe and find the columns whose contents are duplicate.\n",
    "    :param df: Dataframe object\n",
    "    :return: List of columns whose contents are duplicates.\n",
    "    '''\n",
    "    duplicateColumnNames = set()\n",
    "    # Iterate over all the columns in dataframe\n",
    "    for x in range(df.shape[1]):\n",
    "        # Select column at xth index.\n",
    "        col = df.iloc[:, x]\n",
    "        # Iterate over all the columns in DataFrame from (x+1)th index till end\n",
    "        for y in range(x + 1, df.shape[1]):\n",
    "            # Select column at yth index.\n",
    "            otherCol = df.iloc[:, y]\n",
    "            # Check if two columns at x 7 y index are equal\n",
    "            if col.equals(otherCol):\n",
    "                duplicateColumnNames.add(df.columns.values[y])\n",
    " \n",
    "    return list(duplicateColumnNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function readVaa3dFeature(addr, nameF)\n",
    "* **addr** is the location of that plain txt generated by Vaa3D\n",
    "* **nameF** is the name of that feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from IPython.display import display\n",
    "\n",
    "def readVaa3DFeature(addr, nameF):\n",
    "    df = pd.read_csv(addr, sep=r'\\t', engine='python').transpose()\n",
    "    df.rename(columns = df.loc['ID',:], inplace=True)\n",
    "    df.drop(index='ID', inplace = True) \n",
    "    print('Drop duplicate columns '+ str(getDuplicateColumns(df)))\n",
    "    df.drop(columns = getDuplicateColumns(df),inplace = True)\n",
    "    print('Loading ' + str(nameF) +' features successfully, the shape of that dataframe is '+ str(df.shape))\n",
    "    col_mask = df.isnull().any(axis=0) \n",
    "    row_mask = df.isnull().any(axis=1) \n",
    "    if not df.loc[row_mask,col_mask]:\n",
    "        print('NAN value exists for the following feature:')\n",
    "        display(df.loc[row_mask,col_mask])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "def readVaa3DFeature(addr, nameF):\n",
    "    print('Loading ' + str(nameF) +' features') \n",
    "    df = pd.read_csv(addr, sep=r'\\t', header=[0], index_col=[0], delimiter=\"\\t\").transpose()\n",
    "    df.rename(columns={'Number of Bifurcatons':'Number of Bifurcations'}, inplace=True)\n",
    "    use_cols = ['Number of Stems', \n",
    "            'Overall Width', 'Overall Height', 'Overall Depth', \n",
    "            'Total Length', \n",
    "            'Max Euclidean Distance', 'Max Path Distance', \n",
    "            'Number of Bifurcations', 'Number of Branches', 'Number of Tips',\n",
    "            'Max Branch Order','Average Contraction', 'Average Fragmentation',\n",
    "            'Average Bifurcation Angle Local', 'Average Bifurcation Angle Remote', \n",
    "            'Hausdorff Dimension'\n",
    "           ]\n",
    "    df = df[use_cols]\n",
    "    feature_name = use_cols\n",
    "    if nameF == 'axon':\n",
    "        new_feature_name = ['A_'+i.replace(' ', '_') for i in use_cols]\n",
    "    if nameF == 'proximal axon':\n",
    "        new_feature_name = ['AL_'+i.replace(' ', '_') for i in use_cols]\n",
    "    if nameF == 'dendrite':\n",
    "        new_feature_name = ['D_'+i.replace(' ', '_') for i in use_cols]\n",
    "    \n",
    "    df.rename(columns=dict(zip(feature_name, new_feature_name)), inplace=True)\n",
    "\n",
    "    col_mask = df.isnull().any(axis=0) \n",
    "    row_mask = df.isnull().any(axis=1) \n",
    "    if not (df.loc[row_mask,col_mask]).shape == (0,0):\n",
    "        print('NAN value exists for the following feature:')\n",
    "        display(df.loc[row_mask,col_mask])\n",
    "        df.drop(index = df.loc[row_mask,col_mask].index, inplace = True)\n",
    "        print('Related .swc files were removed')\n",
    "    print('Loading successfully, the shape of that dataframe is '+ str(df.shape))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dendrite features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dendrite features\n",
      "NAN value exists for the following feature:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ID</th>\n",
       "      <th>D_Average_Contraction</th>\n",
       "      <th>D_Average_Fragmentation</th>\n",
       "      <th>D_Average_Bifurcation_Angle_Local</th>\n",
       "      <th>D_Average_Bifurcation_Angle_Remote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AA0309</th>\n",
       "      <td>0.814347</td>\n",
       "      <td>58.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AA0411</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ID      D_Average_Contraction  D_Average_Fragmentation  \\\n",
       "AA0309               0.814347                     58.0   \n",
       "AA0411                    NaN                      NaN   \n",
       "\n",
       "ID      D_Average_Bifurcation_Angle_Local  D_Average_Bifurcation_Angle_Remote  \n",
       "AA0309                                NaN                                 NaN  \n",
       "AA0411                                NaN                                 NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related .swc files were removed\n",
      "Loading successfully, the shape of that dataframe is (989, 16)\n"
     ]
    }
   ],
   "source": [
    "dendriteFea = readVaa3DFeature('/home/penglab/FeaCal/Jmorpho_features/dendrite.features/temp', 'dendrite')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deal with data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scale(df, log=True):\n",
    "    scaled_data = np.array(df) / np.sum(df, axis=1).values.reshape(-1,1)\n",
    "    scaled_data[np.isnan(scaled_data)]=0\n",
    "    return scaled_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dendrite features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input neurons: 989\n",
      "Number of input features: 16\n",
      "Loading successfully, the shape of that dataframe is (989, 10)\n"
     ]
    }
   ],
   "source": [
    "lm_dendrite = nmt.features(\"L-measure of dendrite\")\n",
    "lm_dendrite.add_raw_data(dendriteFea)\n",
    "\n",
    "\n",
    "\n",
    "lm_dendrite_df = lm_dendrite.raw_data.copy().loc[:,:]\n",
    "use_cols = [\n",
    "    'D_Number_of_Stems', \n",
    "    'D_Overall_Width', \n",
    "    'D_Overall_Height', \n",
    "    'D_Overall_Depth', \n",
    "    'D_Total_Length',\n",
    "    'D_Max_Euclidean_Distance', \n",
    "    'D_Max_Path_Distance', \n",
    "    'D_Number_of_Branches', \n",
    "    'D_Max_Branch_Order',\n",
    "]\n",
    "lm_dendrite_df = lm_dendrite_df[use_cols]\n",
    "lm_dendrite_df[\"D_Depth_Width-Ratio\"] = lm_dendrite_df[\"D_Overall_Depth\"] / lm_dendrite_df[\"D_Overall_Width\"]\n",
    "\n",
    "\n",
    "col_mask = lm_dendrite_df.isnull().any(axis=0) \n",
    "row_mask = lm_dendrite_df.isnull().any(axis=1) \n",
    "if not (lm_dendrite_df.loc[row_mask,col_mask]).shape == (0,0):\n",
    "    print('NAN value exists for the following feature:')\n",
    "    display(lm_dendrite_df.loc[row_mask,col_mask])\n",
    "    lm_dendrite_df.drop(index = lm_dendrite_df.loc[row_mask,col_mask].index, inplace = True)\n",
    "    print('Related feature samples were removed')\n",
    "print('Loading successfully, the shape of that dataframe is '+ str(lm_dendrite_df.shape))\n",
    "\n",
    "\n",
    "lm_dendrite_df_scale = pd.DataFrame(scale(lm_dendrite_df), \n",
    "                                    index=lm_dendrite_df.index, \n",
    "                                    columns=lm_dendrite_df.columns\n",
    "                                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_dendrite_df_scale.to_excel('/home/penglab/FeaCal/Jmorpho_features/lm_dendrite_df_scale.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate co-clustering matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Using axon morphology as features\n",
    "SETTING CLUSTER NUMBER FROM 8 TO 40\n",
    "\n",
    "### Hierarchy Clustering\n",
    "For Hierarchy method\n",
    "* the most suitable parameter is {'L_method': 'weighted', 'L_metric': 'mahalanobis', 'criterionH': 'distance', 'depth': 2, 'R': None, 't': 0.9, 'optimal_ordering': False}\n",
    "* the ARI is 0.08785414289406115\n",
    "* The setting cluster number's limit is satisfied, the final number of cluster is 8\n",
    "\n",
    "### Kmeans Clustering\n",
    "For Kmeans method\n",
    "* the most suitable parameter is {'n_clusters': 8, 'init': 'k-means++', 'n_init': 21, 'max_iter': 300, 'tol': 0.0001, 'precompute_distances': False, 'verbose': 0, 'random_state': None, 'copy_x': True, 'n_jobs': None, 'algorithm': 'auto'}\n",
    "* the ARI is 0.08343509698163794\n",
    "* The setting cluster number's limit is satisfied, the final number of cluster is 8\n",
    "\n",
    "### DBSCAN Clustering\n",
    "For DBSCAN method\n",
    "* the most suitable parameter is {'eps': 0.31, 'min_samples': 5, 'metric': 'euclidean', 'metric_params': None, 'algorithm': 'auto', 'leaf_size': 30, 'p': None, 'n_jobs': None}\n",
    "* the ARI is 0.10187925011450155\n",
    "* The setting cluster number's limit is satisfied, the final number of cluster is 8\n",
    "\n",
    "### HDBSCAN Clustering\n",
    "For HDBSCAN method, \n",
    "* the most suitable parameter is {'min_cluster_size': 5, 'metric': 'manhattan', 'alpha': 0.8, 'min_samples': 3, 'p': 2, 'algorithm': 'best', 'leaf_size': 40, 'approx_min_span_tree': True, 'gen_min_span_tree': False, 'core_dist_n_jobs': 4, 'cluster_selection_method': 'eom', 'allow_single_cluster': False, 'prediction_data': False, 'match_reference_implementation': False}\n",
    "* the ARI is 0.05177167199451809\n",
    "* The setting cluster number's limit is satisfied, the final number of cluster is 39\n",
    "\n",
    "### SNN Clustering\n",
    "For SNN_community method\n",
    "* the most suitable parameter is {'knn': 5, 'metric': 'minkowski', 'method': 'FastGreedy'}\n",
    "* the ARI is 0.06673692576743263\n",
    "* The setting cluster number's limit is satisfied, the final number of cluster is 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_hier =  {'L_method': 'weighted', 'L_metric': 'mahalanobis', 'criterionH': 'distance', 'depth': 2, 'R': None, \n",
    "             't': 0.9, 'optimal_ordering': False}\n",
    "par_kmeans = {'n_clusters': 8, 'init': 'k-means++', 'n_init': 21, 'max_iter': 300, 'tol': 0.0001, \n",
    "              'precompute_distances': False, 'verbose': 0, 'random_state': None, 'copy_x': True, 'n_jobs': None,\n",
    "              'algorithm': 'auto'}\n",
    "\n",
    "par_dbscan = {'eps': 0.31, 'min_samples': 5, 'metric': 'euclidean', 'metric_params': None, 'algorithm': 'auto', \n",
    "              'leaf_size': 30, 'p': None, 'n_jobs': None}\n",
    "\n",
    "par_hdbscan = {'min_cluster_size': 5, 'metric': 'manhattan', 'alpha': 0.8, 'min_samples': 3, 'p': 2, 'algorithm':\n",
    "               'best', 'leaf_size': 40, 'approx_min_span_tree': True, 'gen_min_span_tree': False, \n",
    "               'core_dist_n_jobs': 4, 'cluster_selection_method': 'eom', 'allow_single_cluster': False, \n",
    "               'prediction_data': False, 'match_reference_implementation': False}\n",
    "\n",
    "par_snn = {'knn':5,'metric':'minkowski','method':'FastGreedy'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(inputUMAP,method='SNN_community',karg_dict={'knn':5, 'metric':'minkowski','method':'FastGreedy'}):\n",
    "    methods_allowed = ['SNN_community', 'Hierarchy', 'Kmeans', 'DBSCAN', 'HDBSCAN']\n",
    "    assert method in methods_allowed, \"Please set 'method' as one of the following: 'SNN_community', 'Hierarchy', 'Kmeans', 'DBSCAN', 'HDBSCAN'\"\n",
    "    selectedUMAP = inputUMAP.copy()\n",
    "    if method=='SNN_community':\n",
    "        #print('Result of SNN_community')\n",
    "        if 'knn' in karg_dict.keys():\n",
    "            knn = karg_dict['knn']\n",
    "        else:\n",
    "            knn = 5\n",
    "        if 'metric' in karg_dict.keys():\n",
    "            metric = karg_dict['metric']\n",
    "        else:\n",
    "            metric = 'minkowski'\n",
    "        if 'method' in karg_dict.keys():\n",
    "            community_method = karg_dict['method']\n",
    "        else:\n",
    "            community_method = 'FastGreedy'\n",
    "        cur_clusters = nmt.get_clusters_SNN_community(selectedUMAP, knn=knn, metric=metric, method=community_method)\n",
    "        \n",
    "\n",
    "    #karg_dict={'L_method':'single','L_metric':'euclidean'.'t':0.9,'criterionH':'inconsistent', depth=2, R=None, monocrit=None}\n",
    "    if method =='Hierarchy':\n",
    "        #print('Result of Hierarchy CLustering')\n",
    "        cur_clusters = nmt.get_clusters_Hierarchy_clustering(selectedUMAP, karg_dict)\n",
    "\n",
    "\n",
    "    if method =='Kmeans':\n",
    "        #print('Result of Kmeans')\n",
    "        cur_clusters = nmt.get_clusters_kmeans_clustering(selectedUMAP, karg_dict)\n",
    "\n",
    "    if method =='DBSCAN':\n",
    "        #print('Result of DBSCAN')\n",
    "        cur_clusters = nmt.get_clusters_dbscan_clustering(selectedUMAP, karg_dict)\n",
    "\n",
    "    if method =='HDBSCAN':\n",
    "        #print('Result of HDBSCAN')\n",
    "        cur_clusters = nmt.get_clusters_hdbscan_clustering(selectedUMAP, karg_dict)\n",
    "    selectedUMAP.loc[:,'Cluster'] = ['C' + str(i) for i in cur_clusters]\n",
    "    return selectedUMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function freq_Matrix(fre_M, cluster_method,para_test)\n",
    "* **fre_M** is the square matrix recording the number of co-clustering\n",
    "* **cluster_method** can be 'Hierarchy','Kmeans', 'DBSCAN','HDBSCAN','SNN_community'\n",
    "* **para_test** is the input parameter dictionary for the cluster method\n",
    "* **iternum** is the number of iteration to generate the coclustering matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import ast\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import time\n",
    "def fre_Matrix(fre_M, cluster_method,para_test):\n",
    "    umapDF = ns.UMAP.iloc[random.sample(range(0,ns.UMAP.shape[0]), int(ns.UMAP.shape[0]*0.95)),:].copy()\n",
    "    resultDF = get_clusters(umapDF.copy(),method =cluster_method,karg_dict = para_test)\n",
    "    Crange, Ccounts = np.unique(resultDF[\"Cluster\"], return_counts = True)\n",
    "    for iter_C in Crange:\n",
    "        selected_row = resultDF[resultDF[\"Cluster\"]==iter_C]\n",
    "        Clist = selected_row.index.tolist()\n",
    "        for sample_row in Clist:\n",
    "            for sample_col in Clist:\n",
    "                fre_M.loc[sample_row,sample_col] =  fre_M.loc[sample_row,sample_col]+1\n",
    "    return fre_M.values\n",
    "def para_cocluster(cluster_method,para_test,corenum, run_num,ns_input):\n",
    "    start = time.perf_counter ()\n",
    "    start=time.time()\n",
    "    cores = corenum#multiprocessing.cpu_count()\n",
    "    pool = multiprocessing.Pool(processes=cores)\n",
    "    fre_M_t = pd.DataFrame(index = ns_input.UMAP.index, columns =ns_input.UMAP.index)\n",
    "    fre_M_t [fre_M_t.isnull()]=0\n",
    "    pool_list=[]\n",
    "    result_list=[]\n",
    "    for i in range(run_num):\n",
    "        pool_list.append(pool.apply_async(fre_Matrix, (fre_M_t, cluster_method, para_test)))\n",
    "        # 这里不能 get， 会阻塞进程\n",
    "\n",
    "    #pool.apply_async之后的语句都是阻塞执行的，\n",
    "    #调用 result.get() 会等待上一个任务执行完之后才会分配下一个任务。\n",
    "    #事实上，获取返回值的过程最好放在进程池回收之后进行，避免阻塞后面的语句。\n",
    "    result_list=[xx.get() for xx in pool_list]\n",
    "    print(sum([xx for xx in  result_list]))\n",
    "    # 最后我们使用一下语句回收进程池:\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    elapsed = (time.time() - start)\n",
    "    print('Time needed to run Hierarchy is '+ str(elapsed))\n",
    "    return sum([xx for xx in  result_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For dendrite morphology features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Using soma morphology as features\n",
    "SETTING CLUSTER NUMBER FROM 8 TO 40\n",
    "\n",
    "### Hierarchy Clustering\n",
    "For Hierarchy method\n",
    "* the most suitable parameter is {'L_method': 'weighted', 'L_metric': 'mahalanobis', 'criterionH': 'distance', 'depth': 2, 'R': None, 't': 0.9, 'optimal_ordering': False}\n",
    "* the ARI is 0.05355799672049869\n",
    "* The setting cluster number's limit is satisfied, the final number of cluster is 8\n",
    "\n",
    "### Kmeans Clustering\n",
    "For Kmeans method\n",
    "* the most suitable parameter is {'n_clusters': 13, 'init': 'random', 'n_init': 12, 'max_iter': 300, 'tol': 0.0001, 'precompute_distances': True, 'verbose': 0, 'random_state': None, 'copy_x': True, 'n_jobs': None, 'algorithm': 'auto'}\n",
    "* the ARI is 0.036079814213255956\n",
    "* The setting cluster number's limit is satisfied, the final number of cluster is 13\n",
    "\n",
    "### DBSCAN Clustering\n",
    "For DBSCAN method\n",
    "* the most suitable parameter is {'eps': 0.22, 'min_samples': 5, 'metric': 'euclidean', 'metric_params': None, 'algorithm': 'auto', 'leaf_size': 30, 'p': None, 'n_jobs': None}\n",
    "* the ARI is 0.039530673163787826\n",
    "* The setting cluster number's limit is satisfied, the final number of cluster is 20\n",
    "\n",
    "### HDBSCAN Clustering\n",
    "For HDBSCAN method, \n",
    "* the most suitable parameter is {'min_cluster_size': 5, 'metric': 'manhattan', 'alpha': 0.9, 'min_samples': 9, 'p': 2, 'algorithm': 'generic', 'leaf_size': 40, 'approx_min_span_tree': True, 'gen_min_span_tree': False, 'core_dist_n_jobs': 4, 'cluster_selection_method': 'eom', 'allow_single_cluster': False, 'prediction_data': False, 'match_reference_implementation': False}\n",
    "* the ARI is 0.020481889417205506\n",
    "* The setting cluster number's limit is satisfied, the final number of cluster is 21\n",
    "\n",
    "### SNN Clustering\n",
    "For SNN_community method\n",
    "* the most suitable parameter is {'knn': 5, 'metric': 'minkowski', 'method': 'FastGreedy'}\n",
    "* the ARI is 0.045224072138369056\n",
    "* The setting cluster number's limit is satisfied, the final number of cluster is 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store UMAP for concated Umap\n"
     ]
    }
   ],
   "source": [
    "lm_dendrite_df_scale= pd.read_excel('/home/penglab/FeaCal/Jmorpho_features/lm_dendrite_df_scale.xlsx', index_col=0)\n",
    "index_origin = ns.metadata.index.tolist()\n",
    "ns.UMAP = nmt.UMAP_wrapper(lm_dendrite_df_scale, n_neighbors=100, min_dist=0.1, n_components=2, metric='euclidean',PCA_first=True,n_PC=100)\n",
    "print('Store UMAP for concated Umap')\n",
    "\n",
    "index_after = ns.UMAP.index.tolist()\n",
    "index_update = [i for i in index_origin if i in index_after ]\n",
    "ns.metadata = ns.metadata.loc[index_update,:]\n",
    "ns.UMAP = ns.UMAP .loc[index_update,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_hier2 =  {'L_method': 'weighted', 'L_metric': 'mahalanobis', 'criterionH': 'distance', 'depth': 2, 'R': None, \n",
    "              't': 0.9, 'optimal_ordering': False}\n",
    "par_kmeans2 = {'n_clusters': 13, 'init': 'random', 'n_init': 12, 'max_iter': 300, 'tol': 0.0001,\n",
    "               'precompute_distances': True, 'verbose': 0, 'random_state': None, 'copy_x': True, 'n_jobs': None,\n",
    "               'algorithm': 'auto'}\n",
    "\n",
    "par_dbscan2 = {'eps': 0.22, 'min_samples': 5, 'metric': 'euclidean', 'metric_params': None, 'algorithm': 'auto', \n",
    "               'leaf_size': 30, 'p': None, 'n_jobs': None}\n",
    "\n",
    "par_hdbscan2 =  {'min_cluster_size': 5, 'metric': 'manhattan', 'alpha': 0.9, 'min_samples': 9, 'p': 2, 'algorithm': 'generic', 'leaf_size': 40, 'approx_min_span_tree': True, 'gen_min_span_tree': False, 'core_dist_n_jobs': 4, 'cluster_selection_method': 'eom', 'allow_single_cluster': False, 'prediction_data': False, 'match_reference_implementation': False}\n",
    "\n",
    "par_snn2 = {'knn':5,'metric':'minkowski','method':'FastGreedy'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DM_hier = para_cocluster('Hierarchy', par_hier2,30, 5000,ns)\n",
    "DM_kmeans = para_cocluster('Kmeans', par_kmeans2,30, 5000,ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DM_hierDF = pd.DataFrame(data=DM_hier, index=ns.UMAP.index, columns=ns.UMAP.index)\n",
    "DM_hierDF.to_excel('/home/penglab/FeaCal/dataSource/denMor/DM_hierDF.xlsx')\n",
    "DM_kmeansDF = pd.DataFrame(data=DM_kmeans, index=ns.UMAP.index, columns=ns.UMAP.index)\n",
    "DM_kmeansDF.to_excel('/home/penglab/FeaCal/dataSource/denMor/DM_kmeansDF.xlsx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4740    0    0 ...    0    0    0]\n",
      " [   0 4705 4481 ...    0    0    0]\n",
      " [   0 4481 4766 ...    0    0    0]\n",
      " ...\n",
      " [   0    0    0 ... 4736    0    0]\n",
      " [   0    0    0 ...    0 4758    0]\n",
      " [   0    0    0 ...    0    0 4745]]\n",
      "Time needed to run Hierarchy is 3709.1338732242584\n"
     ]
    }
   ],
   "source": [
    "DM_dbscan = para_cocluster('DBSCAN', par_dbscan2,30, 5000,ns)\n",
    "DM_dbscanDF = pd.DataFrame(data=DM_dbscan, index=ns.UMAP.index, columns=ns.UMAP.index)\n",
    "DM_dbscanDF = pd.DataFrame(data=DM_dbscan, index=ns.UMAP.index, columns=ns.UMAP.index)\n",
    "DM_dbscanDF.to_excel('/home/penglab/FeaCal/dataSource/denMor/DM_dbscanDF.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4737   26   28 ...    0    0   84]\n",
      " [  26 4734 3910 ...    0   11   26]\n",
      " [  28 3910 4760 ...   17  369   28]\n",
      " ...\n",
      " [   0    0   17 ... 4749   97    0]\n",
      " [   0   11  369 ...   97 4757    3]\n",
      " [  84   26   28 ...    0    3 4766]]\n",
      "Time needed to run Hierarchy is 7875.647224187851\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "DM_hdbscan = para_cocluster('HDBSCAN', par_hdbscan2,30, 5000,ns)\n",
    "\n",
    "DM_hdbscanDF = pd.DataFrame(data=DM_hdbscan, index=ns.UMAP.index, columns=ns.UMAP.index)\n",
    "DM_hdbscanDF.to_excel('/home/penglab/FeaCal/dataSource/denMor/DM_hdbscanDF.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4756    0    0 ...    0    0    2]\n",
      " [   0 4765 4482 ...    0    0    0]\n",
      " [   0 4482 4771 ...    0    0    0]\n",
      " ...\n",
      " [   0    0    0 ... 4762    0    0]\n",
      " [   0    0    0 ...    0 4738    0]\n",
      " [   2    0    0 ...    0    0 4744]]\n",
      "Time needed to run Hierarchy is 5015.162358999252\n"
     ]
    }
   ],
   "source": [
    "DM_snn = para_cocluster('SNN_community', par_snn2,30, 5000,ns)\n",
    "DM_snnDF = pd.DataFrame(data=DM_snn, index=ns.UMAP.index, columns=ns.UMAP.index)\n",
    "DM_snnDF.to_excel('/home/penglab/FeaCal/dataSource/denMor/DM_snnDF.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
