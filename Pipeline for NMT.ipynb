{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data for training purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'neuro_morpho_toolbox' has no attribute 'neuron_set'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e7b026ea2ec4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mneuro_morpho_toolbox\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnmt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneuron_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/penglab/Documents/Janelia_test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'neuro_morpho_toolbox' has no attribute 'neuron_set'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "import neuro_morpho_toolbox as nmt\n",
    "%matplotlib inline\n",
    "ns=nmt.neuron_set('/home/penglab/Documents/Janelia_test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'swc_range6' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e0a2b649f811>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mswc_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mswcname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mswc_range6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0moldname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/penglab/Documents/Janelia_1000/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mswcname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.swc'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mnewname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/penglab/Documents/Janelia_test/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mswcname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.swc'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'swc_range6' is not defined"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "swc_range = []\n",
    "\n",
    "for swcname in swc_range6:\n",
    "    oldname = '/home/penglab/Documents/Janelia_1000/'+str(swcname)+'.swc'\n",
    "    newname = '/home/penglab/Documents/Janelia_test/'+str(swcname)+'.swc'\n",
    "    shutil.copyfile(oldname,newname)\n",
    "for swcname in swc_range7:\n",
    "    oldname = '/home/penglab/Documents/Janelia_1000/'+str(swcname)+'.swc'\n",
    "    newname = '/home/penglab/Documents/Janelia_test/'+str(swcname)+'.swc'\n",
    "    shutil.copyfile(oldname,newname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform UMAP operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for soma location feature, we should flip the coordinates to the same side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-cd499ef25baf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msomaDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'soma_features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mflipF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msize_Z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m11400.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mflipF\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mflipZlist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ns' is not defined"
     ]
    }
   ],
   "source": [
    "somaDF = ns.features['soma_features'].raw_data.copy()\n",
    "flipF = 1\n",
    "size_Z = 11400.0\n",
    "if flipF:\n",
    "    flipZlist=[]\n",
    "    for iterz in somaDF['z']:\n",
    "        if iterz>0.5*size_Z:\n",
    "            flipZlist.append(size_Z-iterz)\n",
    "        else:\n",
    "            flipZlist.append(iterz)\n",
    "    del somaDF['z']\n",
    "    somaDF['z'] = flipZlist\n",
    "upUMAP = nmt.UMAP_wrapper(somaDF, n_neighbors=3, min_dist=0.1, n_components=2, metric='euclidean',PCA_first=True,n_PC=100)\n",
    "if upUMAP.shape == ns.UMAP.shape:\n",
    "    ns.UMAP.loc[:,:]=upUMAP.copy()\n",
    "    print('Replace the unflipped UMAP by a flipped version')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ns.ReduceDimUMAP(feature_set=\"projection_features\", n_neighbors=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the feature set by three kinds of UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_ = ns.ReduceDimUMAP(feature_set=\"soma_features\", n_neighbors=2)\n",
    "_ = ns.FeatureScatter([\"CellType\", \"Hemisphere\", \"Cluster\"], map=\"UMAP\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize measurements for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colname = ['Homogeneity','Completeness','V-measure','Adjusted Rand Index','Adjusted Mutual Information','Silhouette Coefficient','parameter']colname = ['Homogeneity','Completeness','V-measure','Adjusted Rand Index','Adjusted Mutual Information','Silhouette Coefficient','parameter']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the path for storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathStore = '/home/penglab/FeaCal/dataSource/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Store the result of Hierarchy\n",
    "result_hier = pd.DataFrame(columns = colname)\n",
    "L_method_list=['single', 'complete','average','weighted','centroid','median','ward']\n",
    "L_metric_list=['braycurtis', 'canberra', 'chebyshev', 'cityblock', 'correlation', 'cosine',\n",
    "               'dice', 'euclidean', 'hamming', 'jaccard', 'kulsinski', \n",
    "               'mahalanobis', 'matching','minkowski','rogerstanimoto', 'russellrao',\n",
    "               'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean']\n",
    "#linkage using 'yule' and 'jensenshannon' may result in a condensed distance matrix must contain only finite values.\n",
    "\n",
    "criterionH_list=['inconsistent','distance','maxclust','monocrit','maxclust_monocrit']\n",
    "\n",
    "hier_dict={'L_method':'single', 'L_metric':'euclidean','criterionH':'inconsistent', 'depth':2,'R':None,'t':0.9,'optimal_ordering':False}\n",
    "for L_methodidx in L_method_list:\n",
    "    hier_dict.update(L_method = L_methodidx)\n",
    "    for L_metricidx in L_metric_list:\n",
    "        hier_dict.update(L_metric = L_metricidx )\n",
    "        for criterionidx in criterionH_list:\n",
    "            hier_dict.update(criterionH = criterionidx )   \n",
    "            for depth_iter in range(2,15):\n",
    "                hier_dict.update(depth = depth_iter ) \n",
    "                print(hier_dict)\n",
    "                _ = ns. get_clusters(method='Hierarchy',karg_dict=hier_dict)\n",
    "                h = metrics.homogeneity_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "                c = metrics.completeness_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "                V = metrics.v_measure_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "                ARI = metrics.adjusted_rand_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "                AMI = metrics.adjusted_mutual_info_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "                typeR, typeC = np.unique(ns.metadata['Cluster'], return_counts = True)\n",
    "                if len(typeR)<2:\n",
    "                    SS = 'below'\n",
    "                elif len(typeR)>=ns.UMAP.shape[0]:\n",
    "                    SS = 'above'\n",
    "                else:\n",
    "                    SS = metrics.silhouette_score(ns.UMAP, ns.metadata['Cluster'], metric='sqeuclidean')\n",
    "                tempDF = pd.DataFrame([h, c,V,ARI,AMI,SS,str(hier_dict)]).T.copy()\n",
    "                tempDF.columns=colname\n",
    "                result_hier = result_hier.append(tempDF)    \n",
    "idx_hier = ['Hier'+str(x) for x in range(result_hier.shape[0])]    \n",
    "result_hier['idx'] = idx_hier\n",
    "result_hier.set_index('idx',inplace=True)    \n",
    "    \n",
    "result_hier.to_excel(str(pathStore)+'result_hier.xlsx')\n",
    "result_hier = pd.read_excel(str(pathStore)+'result_hier.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "For Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a398803c133c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcolname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Homogeneity'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Completeness'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'V-measure'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Adjusted Rand Index'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Adjusted Mutual Information'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Silhouette Coefficient'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'parameter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#method=['SNN_community','Kmeans'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresult_kmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0minit_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'k-means++'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'random'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "colname = ['Homogeneity','Completeness','V-measure','Adjusted Rand Index','Adjusted Mutual Information','Silhouette Coefficient','parameter']\n",
    "#method=['SNN_community','Kmeans'\n",
    "result_kmeans = pd.DataFrame(columns = colname)\n",
    "\n",
    "init_list=['k-means++','random']\n",
    "algorithm_list = ['auto','full','elkan']\n",
    "precompute_distances_list = ['auto', True, False]\n",
    "n_init_list=['braycurtis', 'canberra', 'chebyshev', 'cityblock', 'correlation', 'cosine',\n",
    "               'dice', 'euclidean', 'hamming', 'jaccard', 'kulsinski', \n",
    "               'mahalanobis', 'matching','minkowski','rogerstanimoto', 'russellrao',\n",
    "               'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean']\n",
    "#linkage using 'yule' and 'jensenshannon' may result in a condensed distance matrix must contain only finite values.\n",
    "\n",
    "criterionH_list='inconsistent','distance','maxclust','monocrit','maxclust_monocrit'\n",
    "\n",
    "kmeans_dict={'n_clusters':20, 'init':'k-means++', 'n_init':10, 'max_iter':300, 'tol':0.0001,'precompute_distances':'auto', 'verbose':0, 'random_state':None,'copy_x': True,'n_jobs':None, 'algorithm':'auto'}\n",
    "\n",
    "for init_idx in init_list:\n",
    "    kmeans_dict.update(init = init_idx)\n",
    "    for algorithm_idx in algorithm_list:\n",
    "        kmeans_dict.update(algorithm = algorithm_idx )\n",
    "        for precompute_distances_idx in precompute_distances_list:\n",
    "            kmeans_dict.update(precompute_distances = precompute_distances_idx )\n",
    "            for n_clustersidx in range(3,25):\n",
    "                kmeans_dict.update(n_clusters = n_clustersidx)     \n",
    "                #Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia.\n",
    "                for n_initidx in range(7,25):\n",
    "                    kmeans_dict.update(n_init = n_initidx) \n",
    "                    print(kmeans_dict)\n",
    "                    _ = ns. get_clusters(method='Kmeans',karg_dict=kmeans_dict)\n",
    "                    h = metrics.homogeneity_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "                    c = metrics.completeness_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "                    V = metrics.v_measure_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "                    ARI = metrics.adjusted_rand_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "                    AMI = metrics.adjusted_mutual_info_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "                    typeR, typeC = np.unique(ns.metadata['Cluster'], return_counts = True)\n",
    "                    if len(typeR)<2:\n",
    "                        SS = 'below'\n",
    "                    elif len(typeR)>=ns.UMAP.shape[0]:\n",
    "                        SS = 'above'\n",
    "                    else:\n",
    "                        SS = metrics.silhouette_score(ns.UMAP, ns.metadata['Cluster'], metric='sqeuclidean')\n",
    "                    \n",
    "                    tempDF = pd.DataFrame([h, c,V,ARI,AMI,SS,str(kmeans_dict)]).T.copy()\n",
    "                    tempDF.columns=colname\n",
    "                    result_kmeans = result_kmeans.append(tempDF)         \n",
    "idx_kmeans = ['KMeans'+str(x) for x in range(result_kmeans.shape[0])]    \n",
    "result_kmeans['idx'] = idx_kmeans\n",
    "result_kmeans.set_index('idx',inplace=True)       \n",
    "result_kmeans.to_excel(str(pathStore)+'result_kmeans.xlsx')\n",
    "result_kmeans = pd.read_excel((str(pathStore)+'result_kmeans.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dbscan = pd.DataFrame(columns = colname)\n",
    "algorithm_list = ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "dbscan_dict={'eps':20, 'min_samples':5, 'metric':'euclidean','metric_params':None, 'algorithm':'auto', 'leaf_size':30, 'p':None,'n_jobs':None}\n",
    "for algorithm_idx in algorithm_list:\n",
    "    dbscan_dict.update(algorithm = algorithm_idx )\n",
    "    for epsidx in np.arange(0,1,0.005)[1:]:\n",
    "        dbscan_dict.update(eps = epsidx)\n",
    "        #print(result_dbscan )\n",
    "        _ = ns. get_clusters(method='DBSCAN',karg_dict=dbscan_dict)\n",
    "        h = metrics.homogeneity_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "        c = metrics.completeness_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "        V = metrics.v_measure_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "        ARI = metrics.adjusted_rand_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "        AMI = metrics.adjusted_mutual_info_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "        typeR, typeC = np.unique(ns.metadata['Cluster'], return_counts = True)\n",
    "        if len(typeR)<2:\n",
    "            SS = 'below'\n",
    "        elif len(typeR)>=ns.UMAP.shape[0]:\n",
    "            SS = 'above'\n",
    "        else:\n",
    "            SS = metrics.silhouette_score(ns.UMAP, ns.metadata['Cluster'], metric='sqeuclidean')\n",
    "        tempDF = pd.DataFrame([h, c,V,ARI,AMI,SS,str(dbscan_dict)]).T.copy()\n",
    "        tempDF.columns=colname\n",
    "        result_dbscan = result_dbscan.append(tempDF)        \n",
    "idx_dbscan = ['DBSCAN'+str(x) for x in range(result_dbscan.shape[0])]    \n",
    "result_dbscan['idx'] = idx_dbscan\n",
    "result_dbscan.set_index('idx',inplace=True)       \n",
    "result_dbscan.to_excel(str(pathStore)+'result_dbscan.xlsx')\n",
    "result_dbscan = pd.read_excel(str(pathStore)+'result_dbscan.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_hdbscan = pd.DataFrame(columns = colname)\n",
    "from sklearn.neighbors.ball_tree import BallTree\n",
    "metric_list = ['euclidean', 'manhattan', 'cityblock', 'l1', 'chebyshev','l2', 'minkowski']\n",
    "#, 'p', 'seuclidean', \n",
    "              # 'mahalanobis', 'wminkowski', 'hamming', 'canberra', 'braycurtis', 'matching', 'jaccard', 'dice', \n",
    "             #  'kulsinski', 'rogerstanimoto', 'russellrao', 'sokalmichener','sokalsneath', 'haversine', 'pyfunc']\n",
    "#['cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan','braycurtis',\n",
    "              # 'canberra', 'chebyshev', 'correlation', 'dice', 'hamming', 'jaccard', \n",
    "             #  'kulsinski', 'mahalanobis', 'matching', 'minkowski','rogerstanimoto', \n",
    "              # 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean']\n",
    "#check use of 'boruvka_balltree' for algorithm later\n",
    "algorithm_list = ['best', 'generic','prims_kdtree', 'boruvka_kdtree']#, \n",
    "cluster_selection_method_list = ['leaf','eom']\n",
    "hdbscan_dict={'min_cluster_size':5, 'metric':'euclidean','alpha':1.0, 'min_samples':1,\n",
    "              'p':2,'algorithm':'best', 'leaf_size':40, 'approx_min_span_tree':True,\n",
    "              'gen_min_span_tree':False,'core_dist_n_jobs':4,'cluster_selection_method':'eom',\n",
    "              'allow_single_cluster': False,'prediction_data':False,\n",
    "              'match_reference_implementation':False}\n",
    "\n",
    "for algorithm_idx in algorithm_list:\n",
    "    hdbscan_dict.update(algorithm = algorithm_idx)\n",
    "    for metric_idx in metric_list:\n",
    "        hdbscan_dict.update(metric = metric_idx)\n",
    "        for cluster_selection_method_idx in cluster_selection_method_list:\n",
    "            hdbscan_dict.update(cluster_selection_method = cluster_selection_method_idx )\n",
    "            for alpha_idx in np.arange(0.8,1.5,0.1):\n",
    "                hdbscan_dict.update(alpha = alpha_idx)\n",
    "                for min_samples_iter in range(1,10):\n",
    "                    hdbscan_dict.update(min_samples = min_samples_iter)\n",
    "            \n",
    "                    #print(hdbscan_dict)\n",
    "                    _ = ns. get_clusters(method='HDBSCAN',karg_dict=hdbscan_dict)\n",
    "                    h = metrics.homogeneity_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "                    c = metrics.completeness_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "                    V = metrics.v_measure_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "                    ARI = metrics.adjusted_rand_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "                    AMI = metrics.adjusted_mutual_info_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "                    typeR, typeC = np.unique(ns.metadata['Cluster'], return_counts = True)\n",
    "                    if len(typeR)<2:\n",
    "                        SS = 'below'\n",
    "                    elif len(typeR)>=ns.UMAP.shape[0]:\n",
    "                        SS = 'above'\n",
    "                    else:\n",
    "                        SS = metrics.silhouette_score(ns.UMAP, ns.metadata['Cluster'], metric='sqeuclidean')\n",
    "                    print(hdbscan_dict)\n",
    "                    tempDF = pd.DataFrame([h, c,V,ARI,AMI,SS,str(hdbscan_dict)]).T.copy()\n",
    "                    tempDF.columns = colname\n",
    "                    result_hdbscan = result_hdbscan.append(tempDF)     \n",
    "                    \n",
    "idx_hdbscan = ['HDBSCAN'+str(x) for x in range(result_hdbscan.shape[0])]    \n",
    "result_hdbscan['idx'] = idx_hdbscan\n",
    "result_hdbscan.set_index('idx',inplace=True)       \n",
    "\n",
    "result_hdbscan.to_excel(str(pathStore)+'result_hdbscan.xlsx')\n",
    "result_hdbscan = pd.read_excel(str(pathStore)+'result_hdbscan.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "snn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_snn= pd.DataFrame(columns = colname)\n",
    "_ = ns. get_clusters(method='SNN_community',\n",
    "                     karg_dict={'knn':5,\n",
    "                                'metric':'minkowski',\n",
    "                                'method':'FastGreedy'})\n",
    "h = metrics.homogeneity_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "c = metrics.completeness_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "V = metrics.v_measure_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "ARI = metrics.adjusted_rand_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "AMI = metrics.adjusted_mutual_info_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "typeR, typeC = np.unique(ns.metadata['Cluster'], return_counts = True)\n",
    "if len(typeR)<2:\n",
    "    SS = 'below'\n",
    "elif len(typeR)>=ns.UMAP.shape[0]:\n",
    "    SS = 'above'\n",
    "else:\n",
    "    SS = metrics.silhouette_score(ns.UMAP, ns.metadata['Cluster'], metric='sqeuclidean')\n",
    "tempDF = pd.DataFrame([h, c,V,ARI,AMI,SS,'default']).T.copy()\n",
    "tempDF.columns=colname\n",
    "result_snn = result_snn.append(tempDF)\n",
    "result_snn.to_excel(str(pathStore)+'result_snn.xlsx')\n",
    "result_snn = pd.read_excel(str(pathStore)+'result_snn.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_hier = pd.read_excel(str(pathStore)+'result_hier.xlsx', index_col=0)\n",
    "result_kmeans = pd.read_excel(str(pathStore)+'result_kmeans.xlsx', index_col=0)\n",
    "result_dbscan = pd.read_excel(str(pathStore)+'result_dbscan.xlsx', index_col=0)\n",
    "result_hdbscan = pd.read_excel(str(pathStore)+'result_hdbscan.xlsx', index_col=0)\n",
    "result_snn = pd.read_excel(str(pathStore)+'result_snn.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## function obtainClusterNum(dflocal,cluster_method,dropF)\n",
    "#### Return number of cluster for each prameter\n",
    "* dflocal is the repeated trial result, containing ['Homogeneity', 'Completeness', 'V-measure', 'Adjusted Rand Index','Adjusted Mutual Information', 'Silhouette Coefficient', 'parameter']\n",
    "* **dropF**: whether of not drop the duplicated rows\n",
    "* Rerun the clustering method and obtain number of cluster in each method\n",
    "* Return a dataframe containing ['Homogeneity', 'Completeness', 'V-measure', 'Adjusted Rand Index','Adjusted Mutual Information', 'Silhouette Coefficient', 'parameter','Cluster Num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def obtainClusterNum(dflocal,cluster_method,dropF):\n",
    "    #result_hier_self=result_hier.copy()\n",
    "    colname = ['Homogeneity','Completeness','V-measure','Adjusted Rand Index','Adjusted Mutual Information','Silhouette Coefficient','parameter']\n",
    "    if dropF:\n",
    "        dflocal_sub = dflocal.iloc[:,:-1].drop_duplicates()\n",
    "        indexls = dflocal_sub.index\n",
    "    else:\n",
    "        indexls = dflocal.index\n",
    "\n",
    "    df = pd.DataFrame(columns = colname,index = indexls)\n",
    "    df['Cluster Num'] = 0\n",
    "    df['parameter'] = dflocal.loc[indexls,'parameter']\n",
    "    for iter_idx in df.index:\n",
    "        _ = ns. get_clusters(method = cluster_method,karg_dict = ast.literal_eval(df.loc[iter_idx]['parameter']))\n",
    "        h = metrics.homogeneity_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "        c = metrics.completeness_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "        V = metrics.v_measure_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "        ARI = metrics.adjusted_rand_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "        AMI = metrics.adjusted_mutual_info_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "        typeR, typeC = np.unique(ns.metadata['Cluster'], return_counts = True)\n",
    "        if len(typeR)<2:\n",
    "            SS = 'below'\n",
    "        elif len(typeR)>=ns.UMAP.shape[0]:\n",
    "            SS = 'above'\n",
    "        else:\n",
    "            SS = metrics.silhouette_score(ns.UMAP, ns.metadata['Cluster'], metric='sqeuclidean')\n",
    "        df.loc[iter_idx,'Homogeneity'] = h\n",
    "        df.loc[iter_idx,'Completeness'] = c\n",
    "        df.loc[iter_idx,'V-measure'] = V\n",
    "        df.loc[iter_idx,'Adjusted Rand Index'] = ARI\n",
    "        df.loc[iter_idx,'Adjusted Mutual Information'] = AMI\n",
    "        df.loc[iter_idx,'Silhouette Coefficient'] = SS\n",
    "        typeR, typeC = np.unique(ns.metadata['Cluster'], return_counts = True)\n",
    "        #print(hdbscan_dict)\n",
    "        df.loc[iter_idx,'Cluster Num'] = len(typeR)\n",
    "                        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### unique_* is a dataframe containing the unique rows of the original trails, it also record number of cluster under that parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_hier = obtainClusterNum(result_hier.copy(),'Hierarchy',1)\n",
    "unique_hier.to_excel(str(pathStore)+'unique_hier.xlsx')\n",
    "unique_hier = pd.read_excel(str(pathStore)+'unique_hier.xlsx', index_col=0)\n",
    "unique_kmeans = obtainClusterNum(result_kmeans.copy(),'Kmeans',1)\n",
    "unique_kmeans.to_excel(str(pathStore)+'unique_kmeans.xlsx')\n",
    "unique_kmeans = pd.read_excel(str(pathStore)+'unique_kmeans.xlsx', index_col=0)\n",
    "unique_dbscan = obtainClusterNum(result_dbscan.copy(),'DBSCAN',1)\n",
    "unique_dbscan.to_excel(str(pathStore)+'unique_dbscan.xlsx')\n",
    "unique_dbscan = pd.read_excel(str(pathStore)+'unique_dbscan.xlsx', index_col=0)\n",
    "unique_hdbscan = obtainClusterNum(result_hdbscan.copy(),'HDBSCAN',1)\n",
    "unique_hdbscan.to_excel(str(pathStore)+'unique_hdbscan.xlsx')\n",
    "unique_hdbscan = pd.read_excel(str(pathStore)+'unique_hdbscan.xlsx', index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the box plot for different clustering method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "pd.set_option('max_colwidth',300)\n",
    "def boxCLUSTER(unique_df,cluster_method):\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    ax1.set_title(cluster_method)\n",
    "    ax1.boxplot(unique_df['Cluster Num'])    \n",
    "boxCLUSTER(unique_hier,'Hierarchy clustering')\n",
    "boxCLUSTER(unique_kmeans,'KMeans clustering')\n",
    "boxCLUSTER(unique_dbscan,'DBSCAN clustering')\n",
    "boxCLUSTER(unique_hdbscan,'HDBSCAN clustering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot each kind of clustering result separately\n",
    "\n",
    "* Each line corresponds to one column\n",
    "* Show the fluctuation of each method using different parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_col(dflocal,cluster_name,dropF):\n",
    "    #result_hier_self=result_hier.copy()\n",
    "    if dropF:\n",
    "        df = dflocal.iloc[:,:-1].drop_duplicates()\n",
    "    else:\n",
    "        df = dflocal.copy()\n",
    "    df = df[df['Silhouette Coefficient']!='below'].copy()\n",
    "    df = df[df['Silhouette Coefficient']!='above'].copy()\n",
    "    col_Homogeneity = df['Homogeneity']\n",
    "    col_Completeness = df['Completeness']\n",
    "    col_Vmeasure= df['V-measure']\n",
    "    col_ARI = df['Adjusted Rand Index']\n",
    "    col_AMI = df['Adjusted Mutual Information']\n",
    "    col_SC = df['Silhouette Coefficient']\n",
    "    \n",
    "    x_axix=range(1,df.shape[0]+1)\n",
    "\n",
    "    plt.title('Value fluctuation for '+str(cluster_name)+'with '+str(df.shape[0])+' unique trials')\n",
    "    plt.plot(x_axix, col_Homogeneity.tolist(), color='green', label='Homogeneity')\n",
    "    plt.plot(x_axix, col_Completeness.tolist(), color='brown', label='Completeness')\n",
    "    plt.plot(x_axix, col_Vmeasure.tolist(),  color='skyblue', label='V-measure')\n",
    "    plt.plot(x_axix, col_ARI.tolist(), color='blue', label='Adjusted Rand Index')\n",
    "    plt.plot(x_axix, col_AMI.tolist(), color='magenta', label='Adjusted Mutual Information')\n",
    "    plt.plot(x_axix, col_SC.tolist(), color='pink', label='Silhouette Coefficient')\n",
    "    plt.legend() # 显示图例\n",
    "    #a=['','H', 'C', 'V', 'ARI', 'AMI', 'SC']\n",
    "    #plt.xticks(arange(len(a)),a)\n",
    "    plt.xlabel('Different Parameter Choices')\n",
    "    plt.ylabel('metric')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "plot_col(result_hier.copy(),'Hierarchy clustering ',1)\n",
    "\n",
    "plot_col(result_kmeans.copy(),'Kmeans clustering',1)\n",
    "\n",
    "plot_col(result_dbscan.copy(),'DBSCAN clustering',1)\n",
    "\n",
    "plot_col(result_hdbscan.copy(),'HDBSCAN clustering',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To compare all the results for different metirc\n",
    "### Detail\n",
    "* metric can be 'Homogeneity','Completeness','V-measure','Adjusted Rand Index','Adjusted Mutual Information','Silhouette Coefficient'\n",
    "* Delete clustering result leading to cluster less than 2 or # of cluster = # of swc files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The unique_* below is a dataframe containing the unique rows of the original trails, it also record number of cluster under that parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add SNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_snn= pd.DataFrame(columns =['Homogeneity', 'Completeness', 'V-measure', 'Adjusted Rand Index',\n",
    "       'Adjusted Mutual Information', 'Silhouette Coefficient', 'parameter',\n",
    "       'Cluster Num'],index=['snn0'])\n",
    "_ = ns. get_clusters(method='SNN_community',\n",
    "                     karg_dict={'knn':5,\n",
    "                                'metric':'minkowski',\n",
    "                                'method':'FastGreedy'})\n",
    "h = metrics.homogeneity_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "c = metrics.completeness_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "V = metrics.v_measure_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "ARI = metrics.adjusted_rand_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "AMI = metrics.adjusted_mutual_info_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "typeR, typeC = np.unique(ns.metadata['Cluster'], return_counts = True)\n",
    "if len(typeR)<2:\n",
    "    SS = 'below'\n",
    "elif len(typeR)>=ns.UMAP.shape[0]:\n",
    "    SS = 'above'\n",
    "else:\n",
    "    SS = metrics.silhouette_score(ns.UMAP, ns.metadata['Cluster'], metric='sqeuclidean')\n",
    "result_snn.loc['snn0','Homogeneity']= h\n",
    "result_snn.loc['snn0','Completeness']= c\n",
    "result_snn.loc['snn0','V-measure']=V\n",
    "result_snn.loc['snn0','Adjusted Rand Index']=ARI\n",
    "result_snn.loc['snn0','Adjusted Mutual Information']=AMI\n",
    "result_snn.loc['snn0','Silhouette Coefficient']=SS\n",
    "result_snn.loc['snn0','parameter']=str({'knn':5,\n",
    "                                'metric':'minkowski',\n",
    "                                'method':'FastGreedy'})\n",
    "result_snn.loc['snn0','Cluster Num']=len(typeR)\n",
    "unique_snn = result_snn.copy()\n",
    "unique_snn.to_excel(str(pathStore)+'unique_snn.xlsx')\n",
    "unique_snn = pd.read_excel(str(pathStore)+'unique_snn.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Look into the top ten rows which maximizing ARI\n",
    "### function maxARI(uniqueDF,cluster_method,swcDF,metric,flipF,size_Z)\n",
    "* **uniqueDF** is the dataframe with unrepeated parameters\n",
    "* **cluster_method** can be 'Hierarchy','Kmeans', 'DBSCAN','HDBSCAN','' \n",
    "* **swcDF** is the dataframe containing the coordinate of the soma\n",
    "* **metric** can be 'braycurtis', 'canberra', 'chebyshev', 'cityblock', 'correlation', 'cosine', 'dice', 'euclidean', 'hamming', 'jaccard', 'jensenshannon', 'kulsinski', 'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule'\n",
    "\n",
    "* **flipF**: \n",
    "    * if 1, will flip all the soma to the same hemishphere\n",
    "* **size_Z** the size of z axis of the brain\n",
    "* **clusterT** the minimum and maximum threshold for number of cluster\n",
    "* **somaDF** is the dataframe containing the region of soma: ns.features['soma_features'].region\n",
    "* **axonDF** is the dataframe containing the region of axon: ns.features['projection_features'].raw_data\n",
    "* **normaF** is the indicator for whether normalize the result or not.\n",
    "* **Ffromto** is the indicator for whether return the from-to dataframe for chord diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import matplotlib.pyplot as plt\n",
    "def maxARIdic(uniqueDF,cluster_method,swcDF,flipF,size_Z,clusterT,somaDF,axonDF,normaF,Ffromto):\n",
    "    #if the setting cluster is not satisfied, will print the final number of cluster\n",
    "    printF=0\n",
    "    [min_Clusternum,max_Clusternum]=clusterT\n",
    "    somaregionDF = somaDF.copy()\n",
    "    axonregionDF = axonDF.copy()            \n",
    "    df_pre = uniqueDF.copy()\n",
    "    df_pre = df_pre[df_pre['Silhouette Coefficient']!='below'].copy()\n",
    "    df_pre = df_pre[df_pre['Silhouette Coefficient']!='above'].copy()\n",
    "    df_pre = df_pre[df_pre['Completeness']!=1].copy()\n",
    "    df_pre = df_pre[df_pre['Adjusted Rand Index']>0].copy()\n",
    "    df_pre.sort_values('Adjusted Rand Index',inplace=True,ascending=False)\n",
    "    df_pre = df_pre[df_pre['Cluster Num']<=max_Clusternum].copy()\n",
    "    df_pre = df_pre[df_pre['Cluster Num']>=min_Clusternum].copy()\n",
    "\n",
    "    #under each parameter\n",
    "    if df_pre.shape[0]==0:\n",
    "        printF = 1\n",
    "        print('************\\n')\n",
    "        print('The input cluster number range is not suitable, just from the swc files which corresponds to the min 40 cluster number and selected the one maximizing ARI')\n",
    "        df_pre = uniqueDF.copy()\n",
    "        df_pre = df_pre[df_pre['Silhouette Coefficient']!='below'].copy()\n",
    "        df_pre = df_pre[df_pre['Silhouette Coefficient']!='above'].copy()\n",
    "        df_pre = df_pre[df_pre['Completeness']!=1].copy()\n",
    "        df_pre.sort_values('Adjusted Rand Index',inplace=True,ascending=False)\n",
    "        df_pre = df_pre[df_pre['Adjusted Rand Index']>0].copy()\n",
    "        df_pre = df_pre[df_pre['Cluster Num']<=max_Clusternum+20].copy()\n",
    "        df_pre = df_pre[df_pre['Cluster Num']>=min_Clusternum].copy()\n",
    "        df_pre = df_pre.iloc[:min(20,df_pre.shape[0]),:].copy()\n",
    "        df_pre.sort_values('Cluster Num',inplace=True,ascending=True)\n",
    "    if cluster_method == 'SNN_community':\n",
    "        df_pre = uniqueDF.copy()\n",
    "    iter_idx = df_pre.index[0]\n",
    "    _ = ns. get_clusters(method = cluster_method,karg_dict = ast.literal_eval(df_pre.loc[iter_idx]['parameter']))\n",
    "    print('For '+str(cluster_method) +' method, the most suitable parameter is '+str(df_pre.loc[iter_idx]['parameter']))\n",
    "    somaregionDF['cluster'] = ns.metadata['Cluster']\n",
    "    axonregionDF['cluster'] = ns.metadata['Cluster']\n",
    "    V = metrics.v_measure_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "    ARI = metrics.adjusted_rand_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "    print('For '+str(cluster_method) +' method, the ARI is '+str(ARI))\n",
    "    AMI = metrics.adjusted_mutual_info_score(ns.metadata['CellType'],ns.metadata['Cluster'])\n",
    "    typeR, typeC = np.unique(ns.metadata['Cluster'], return_counts = True)\n",
    "    if printF:\n",
    "        print('The setting cluster number\\'s limit is not satisfied, the final number of cluster is '+str(len(typeR)))\n",
    "    else:\n",
    "        print('The setting cluster number\\'s limit is satisfied, the final number of cluster is '+str(len(typeR)))\n",
    "    if cluster_method == 'HDBSCAN':\n",
    "        delist=np.array([])\n",
    "        for itertype in typeR:\n",
    "            if itertype[1]=='-':\n",
    "                delist = np.append(delist,itertype)\n",
    "            if itertype[1]=='0':\n",
    "                delist=np.append(delist,itertype)\n",
    "        detailDF = pd.DataFrame(index = np.setdiff1d(typeR,delist),columns = ['swc list','soma region',\n",
    "                                                                              'max soma','total soma',\n",
    "                                                                              'max axon'])\n",
    "    else:\n",
    "        detailDF = pd.DataFrame(index = typeR,columns = ['swc list','soma region','max soma','total soma',\n",
    "                                                         'max axon'])\n",
    "    #\n",
    "    #detailDF.loc[:,'Cluster Type'] = detailDF.index\n",
    "    if len(typeR)<2:\n",
    "        SS = 'below'\n",
    "    elif len(typeR)>=ns.UMAP.shape[0]:\n",
    "        SS = 'above'\n",
    "    else:\n",
    "        SS = metrics.silhouette_score(ns.UMAP, ns.metadata['Cluster'], metric='sqeuclidean')\n",
    "        #LABEL LIST CAN BE DETELETED LATER\n",
    "\n",
    "\n",
    "    # for each cluster result UNDER EACH PARAMETER\n",
    "    somaCLUSTERlist = []\n",
    "    axonCLUSTERlist = []\n",
    "\n",
    "    #axonCLUSTER = pd.DataFrame(index = axonregionDF.columns[:-1])\n",
    "    for label_idx in detailDF.index:\n",
    "        axonTEMPdf = pd.DataFrame(index = axonregionDF.columns[:-1])\n",
    "        swcINlabel = somaregionDF[somaregionDF['cluster']==label_idx]\n",
    "        detailDF.loc[label_idx,'swc list'] = (swcINlabel.index).tolist()\n",
    "        #assert(len(inter_list)==somaDF[somaDF['cluster'] !=label_idx].shape[0])\n",
    "        detailDF.loc[label_idx,'soma region'] = swcINlabel['Region'].tolist()\n",
    "        del_soma_region=detailDF.loc[label_idx,'soma region']\n",
    "        while 'unknown' in del_soma_region:\n",
    "            del_soma_region.remove('unknown')\n",
    "            del_soma_region.append('others')\n",
    "        while 'fiber tracts' in del_soma_region:\n",
    "            del_soma_region.remove('fiber tracts')\n",
    "            del_soma_region.append('others')\n",
    "        IDrange, IDcounts = np.unique(del_soma_region, return_counts = True)\n",
    "        #sort the counting\n",
    "        count_sort_ind = np.argsort(-IDcounts)\n",
    "        dicitem = []\n",
    "        for i in range(1,min(len(IDrange)+1,4)):\n",
    "            dicitem.append(str(i)+'somaR')\n",
    "        for i in range(1,min(len(IDrange)+1,4)):\n",
    "            dicitem.append(str(i)+'somaC')\n",
    "        #select the first three regions\n",
    "        dicvalue = IDrange[count_sort_ind][0:min(3,len(IDrange))]\n",
    "        dicvalue = np.append(dicvalue,IDcounts[count_sort_ind][0:min(3,len(IDrange))])\n",
    "        detailDF.loc[label_idx,'max soma'] = str(dict(zip(dicitem, dicvalue)))\n",
    "        somaCLUSTERlist.append(detailDF.loc[label_idx,'max soma'] )\n",
    "        detailDF.loc[label_idx,'total soma'] = swcINlabel.shape[0]\n",
    "\n",
    "        #axonLABEL stores all the rows under same cluster\n",
    "        axonLABEL = axonregionDF[axonregionDF['cluster']==label_idx].copy()\n",
    "        axonLABEL.drop('cluster', axis=1,inplace=True)\n",
    "        axonTEMPdf[label_idx]=axonLABEL.sum()\n",
    "        if 'contra_fiber tracts' in axonTEMPdf.index:\n",
    "            axonTEMPdf.drop(index=['contra_fiber tracts'],inplace = True)\n",
    "        if 'ipsi_fiber tracts' in axonTEMPdf.index:\n",
    "            axonTEMPdf.drop(index=['ipsi_fiber tracts'],inplace = True)\n",
    "        if normaF:\n",
    "            #normalize the sum\n",
    "            axonTEMPdf = normalize(axonTEMPdf , log=True)\n",
    "        axonTEMPdf.sort_values(by=label_idx, ascending=False, inplace=True)\n",
    "        dicitem = []\n",
    "        for i in range(1,4):\n",
    "            dicitem.append(str(i)+'axonR')\n",
    "        for i in range(1,4):\n",
    "            dicitem.append(str(i)+'axonC')\n",
    "        dicvalue = axonTEMPdf.index[:3]\n",
    "        dicvalue = np.append(dicvalue,axonTEMPdf.loc[:,label_idx][:3])\n",
    "        detailDF.loc[label_idx,'max axon'] = str(dict(zip(dicitem, dicvalue)))\n",
    "        axonCLUSTERlist.append(detailDF.loc[label_idx,'max axon'] )\n",
    "    if not Ffromto:\n",
    "        return detailDF\n",
    "    else:\n",
    "        clusterDF=detailDF.copy()\n",
    "        dfname=[]\n",
    "        for cluster_idx in clusterDF.index:\n",
    "            dfname.append((str(cluster_idx) + 'DF'))\n",
    "\n",
    "        #generate a dataframe for each cluster\n",
    "        dicDF = {k:None for k in dfname}\n",
    "\n",
    "        #iterate the correponding dataframe and cluster at the same time\n",
    "        for key_iter,cluster_idx in zip(dicDF.keys(),clusterDF.index):\n",
    "            dic_soma = ast.literal_eval(clusterDF.loc[cluster_idx,'max soma'])\n",
    "            iterL = int(len(dic_soma.keys())/2)\n",
    "            somaNAME = []\n",
    "            somaCOUNT = []\n",
    "\n",
    "            for i in range(1,int(iterL)+1):\n",
    "                somaNAME.append(dic_soma[(str(i)+'somaR')])\n",
    "            for i in range(1,iterL+1):\n",
    "                somaCOUNT.append(dic_soma[str(i)+'somaC'])\n",
    "            somaArr = np.array(somaCOUNT, dtype = float)\n",
    "            somaArr = somaArr/sum(somaArr)\n",
    "            dic_axon = ast.literal_eval(clusterDF.loc[cluster_idx,'max axon'])\n",
    "            iterL = int(len(dic_axon.keys())/2)\n",
    "            axonNAME = []\n",
    "            axonCOUNT = []\n",
    "            for i in range(1,iterL+1):\n",
    "                axonNAME.append(dic_axon[(str(i)+'axonR')])\n",
    "            for i in range(1,iterL+1):\n",
    "                axonCOUNT.append(dic_axon[str(i)+'axonC']) \n",
    "\n",
    "            tempDF = pd.DataFrame()\n",
    "            for row_soma in somaNAME:\n",
    "                for col_axon,len_axon in zip(axonNAME,axonCOUNT):\n",
    "                    tempDF.loc[row_soma, col_axon] = len_axon\n",
    "\n",
    "            assert(tempDF.shape[0] == len(somaArr))  \n",
    "            tempDF=tempDF.astype(float)\n",
    "            for i in range(tempDF.shape[0]):\n",
    "                tempDF.iloc[i,:] = tempDF.iloc[i,:]*somaArr[i].copy()\n",
    "\n",
    "            newDF = pd.DataFrame(columns=['from','to','value'],index=range(tempDF.shape[0]*tempDF.shape[1]))\n",
    "            newDF.loc[:,'from'] = (tempDF.index).tolist()*len(tempDF.columns)\n",
    "            newDF.loc[:,'to'] = [ele for ele in (tempDF.columns).tolist() for _ in range(len(tempDF.index))]\n",
    "            valuelist=[]\n",
    "            for iter_col in tempDF.columns:\n",
    "                for iter_row in tempDF.index:\n",
    "                    valuelist.append(tempDF.loc[iter_row,iter_col])\n",
    "            newDF.loc[:,'value']=valuelist\n",
    "            dicDF[key_iter]= newDF.copy()\n",
    "    return dicDF\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following obtained the first 10 rows which maximizing ARI\n",
    "\n",
    "#### Only count the nearst 400 inter-cluster soma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_as = maxARIdic(unique_hier,'Hierarchy',ns.features['soma_features'].raw_data.copy(),1,11400.0,[8,40],\n",
    "                    ns.features['soma_features'].region,ns.features['projection_features'].raw_data,1,0)\n",
    "hier_as.to_excel('/home/penglab/FeaCal/dataSource/somaLoc/hier_as.xlsx')\n",
    "hier_as=pd.read_excel('/home/penglab/FeaCal/dataSource/somaLoc/hier_as.xlsx', index_col=0)\n",
    "dicHIER= maxARIdic(unique_hier,'Hierarchy',ns.features['soma_features'].raw_data.copy(),1,11400.0,[8,40],\n",
    "                    ns.features['soma_features'].region,ns.features['projection_features'].raw_data,1,1)\n",
    "for iter_key in dicHIER.keys():\n",
    "    tempDF = dicHIER[iter_key]\n",
    "    #print(tempDF)\n",
    "    #(tempDF).to_csv(str(pathStore)+'HIER/'+str(iter_key)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_as = maxARIdic(unique_kmeans,'Kmeans',ns.features['soma_features'].raw_data.copy(),1,11400.0,[8,40],\n",
    "                    ns.features['soma_features'].region,ns.features['projection_features'].raw_data,1,0)\n",
    "kmeans_as.to_excel('/home/penglab/FeaCal/dataSource/somaLoc/kmeans_as.xlsx')\n",
    "kmeans_as=pd.read_excel('/home/penglab/FeaCal/dataSource/somaLoc/kmeans_as.xlsx', index_col=0)\n",
    "dicKMEANS= maxARIdic(unique_kmeans,'Kmeans',ns.features['soma_features'].raw_data.copy(),1,11400.0,[8,40],\n",
    "                    ns.features['soma_features'].region,ns.features['projection_features'].raw_data,1,1)\n",
    "for iter_key in dicKMEANS.keys():\n",
    "    tempDF = dicKMEANS[iter_key]\n",
    "    print(tempDF)\n",
    "    #(tempDF).to_csv(str(pathStore)+'KMEANS/'+str(iter_key)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_as = maxARIdic(unique_dbscan,'DBSCAN',ns.features['soma_features'].raw_data.copy(),1,11400.0,[8,40],\n",
    "                    ns.features['soma_features'].region,ns.features['projection_features'].raw_data,1,0)\n",
    "dbscan_as.to_excel('/home/penglab/FeaCal/dataSource/somaLoc/dbscan_as.xlsx')\n",
    "dbscan_as=pd.read_excel('/home/penglab/FeaCal/dataSource/somaLoc/dbscan_as.xlsx', index_col=0)\n",
    "dicDBSCAN= maxARIdic(unique_dbscan,'DBSCAN',ns.features['soma_features'].raw_data.copy(),1,11400.0,[8,40],\n",
    "                    ns.features['soma_features'].region,ns.features['projection_features'].raw_data,1,1)\n",
    "for iter_key in dicDBSCAN.keys():\n",
    "    tempDF = dicDBSCAN[iter_key]\n",
    "    print(tempDF)\n",
    "    #(tempDF).to_csv(str(pathStore)+'DBSCAN/'+str(iter_key)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_as = maxARIdic(unique_hdbscan,'HDBSCAN',ns.features['soma_features'].raw_data.copy(),1,11400.0,[8,40],\n",
    "                    ns.features['soma_features'].region,ns.features['projection_features'].raw_data,1,0)\n",
    "hdbscan_as.to_excel('/home/penglab/FeaCal/dataSource/somaLoc/hdbscan_as.xlsx')\n",
    "hdbscan_as=pd.read_excel('/home/penglab/FeaCal/dataSource/somaLoc/dbscan_as.xlsx', index_col=0)\n",
    "dicHDBSCAN= maxARIdic(unique_hdbscan,'HDBSCAN',ns.features['soma_features'].raw_data.copy(),1,11400.0,[8,40],\n",
    "                      ns.features['soma_features'].region,ns.features['projection_features'].raw_data,1,1)\n",
    "for iter_key in dicHDBSCAN.keys():\n",
    "    tempDF = dicHDBSCAN[iter_key]\n",
    "    print(tempDF)\n",
    "    #(tempDF).to_csv(str(pathStore)+'HDBSCAN/'+str(iter_key)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn_as = maxARIdic(unique_snn,'SNN_community',ns.features['soma_features'].raw_data.copy(),1,11400.0,[8,40],\n",
    "                    ns.features['soma_features'].region,ns.features['projection_features'].raw_data,1,0)\n",
    "snn_as.to_excel('/home/penglab/FeaCal/dataSource/somaLoc/snn_as.xlsx')\n",
    "snn_as=pd.read_excel('/home/penglab/FeaCal/dataSource/somaLoc/snn_as.xlsx', index_col=0)\n",
    "dicSNN= maxARIdic(unique_snn,'SNN_community',ns.features['soma_features'].raw_data.copy(),1,11400.0,[8,40],\n",
    "                      ns.features['soma_features'].region,ns.features['projection_features'].raw_data,1,1)\n",
    "for iter_key in dicHDBSCAN.keys():\n",
    "    tempDF = dicHDBSCAN[iter_key]\n",
    "    print(tempDF)\n",
    "    #(tempDF).to_csv(str(pathStore)+'SNN/'+str(iter_key)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "colorlist=[\"#7FC97F\",\"#BEAED4\",\"#FDC086\",\"#FFFF99\",\"#386CB0\",\"#F0027F\",\"#BF5B17\",\"#666666\",\"#1B9E77\",\"#D95F02\",\n",
    "           \"#7570B3\",\"#E7298A\",\"#66A61E\",\"#E6AB02\",\"#A6761D\",\"#666666\",\"#A6CEE3\",\"#1F78B4\",\"#B2DF8A\",\"#33A02C\",\n",
    "           \"#FB9A99\",\"#E31A1C\",\"#FDBF6F\",\"#FF7F00\",\"#CAB2D6\",\"#6A3D9A\",\"#FFFF99\",\"#B15928\",\"#FBB4AE\",\"#B3CDE3\",\n",
    "           \"#CCEBC5\",\"#DECBE4\",\"#FED9A6\",\"#FFFFCC\",\"#E5D8BD\",\"#FDDAEC\",\"#F2F2F2\",\"#B3E2CD\",\"#FDCDAC\",\"#CBD5E8\",\n",
    "           \"#F4CAE4\",\"#E6F5C9\",\"#FFF2AE\",\"#F1E2CC\",\"#CCCCCC\",\"#E41A1C\",\"#377EB8\",\"#4DAF4A\",\"#984EA3\",\"#FF7F00\",\n",
    "           \"#FFFF33\",\"#A65628\",\"#F781BF\",\"#999999\",\"#66C2A5\",\"#FC8D62\",\"#8DA0CB\",\"#E78AC3\",\"#A6D854\",\"#FFD92F\",\n",
    "           \"#E5C494\",\"#B3B3B3\",\"#8DD3C7\",\"#FFFFB3\",\"#BEBADA\",\"#FB8072\",\"#80B1D3\",\"#FDB462\",\"#B3DE69\",\"#FCCDE5\",\n",
    "           \"#D9D9D9\",\"#BC80BD\",\"#CCEBC5\"]\n",
    "lut = dict(zip(ns.metadata['CellType'].unique(), colorlist))\n",
    "row_colors = ns.metadata['CellType'].map(lut)\n",
    "row_colors[row_colors.isnull()]='white'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot cocluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.spatial import distance\n",
    "from scipy.cluster import hierarchy\n",
    "import numpy as np\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster,inconsistent\n",
    "#print the cocluster result\n",
    "#output the linkage matrix\n",
    "def bestCOCLUSTER(df):\n",
    "    linkmethod = ['single', 'complete','average','weighted','centroid','median','ward']\n",
    "    paraDF = pd.DataFrame(columns =['method','CCC'],index = linkmethod)\n",
    "    paraDF.loc[:,'method'] = linkmethod\n",
    "    \n",
    "    for iter_m in linkmethod:\n",
    "        Y = distance.pdist(np.asarray(df))\n",
    "        Z = hierarchy.linkage(Y, method = iter_m)\n",
    "        c, coph_dists = hierarchy.cophenet(Z,Y)\n",
    "        paraDF.loc[iter_m,'CCC'] = c\n",
    "    paraDF.sort_values(by='CCC', ascending = False, inplace = True)\n",
    "    #print(paraDF)\n",
    "    row_linkage = hierarchy.linkage(distance.pdist(np.asarray(df)), method = paraDF.iloc[0,0])\n",
    "\n",
    "    col_linkage = hierarchy.linkage(distance.pdist(np.asarray(df).T), method = paraDF.iloc[0,0])\n",
    "    #print(paraDF.iloc[0,0])\n",
    "    sns.clustermap(df, row_linkage = row_linkage, col_linkage = col_linkage, row_colors=row_colors ,col_colors = row_colors, figsize=(13, 13))#, cmap=sns.diverging_palette(h_neg=150, h_pos=275, s=80, l=55, as_cmap=True))    \n",
    "    return hierarchy.linkage(distance.pdist(np.asarray(df)))\n",
    "\n",
    "\n",
    "\n",
    "def coclusterResult(Z_sample):\n",
    "    t = input('Please input the number of cluster: ')\n",
    "    return fcluster(Z_sample,t,criterion='maxclust')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set neuron_set.metadata['Cluster']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From featureDF we can generate the corresponding UMAP, then we will use the coclustering result to perfrom fcluster, then set the ['cluster']result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDuplicateColumns(df):\n",
    "    '''\n",
    "    Get a list of duplicate columns.\n",
    "    It will iterate over all the columns in dataframe and find the columns whose contents are duplicate.\n",
    "    :param df: Dataframe object\n",
    "    :return: List of columns whose contents are duplicates.\n",
    "    '''\n",
    "    duplicateColumnNames = set()\n",
    "    # Iterate over all the columns in dataframe\n",
    "    for x in range(df.shape[1]):\n",
    "        # Select column at xth index.\n",
    "        col = df.iloc[:, x]\n",
    "        # Iterate over all the columns in DataFrame from (x+1)th index till end\n",
    "        for y in range(x + 1, df.shape[1]):\n",
    "            # Select column at yth index.\n",
    "            otherCol = df.iloc[:, y]\n",
    "            # Check if two columns at x 7 y index are equal\n",
    "            if col.equals(otherCol):\n",
    "                duplicateColumnNames.add(df.columns.values[y])\n",
    " \n",
    "    return list(duplicateColumnNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* scaleData is the dataframe containing features extracted from Vaa3D\n",
    "* Z_Sample is the condensed distance matrix from cocluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def generateNS(featureDF,ns_input,Z_sample,storePath,outname):\n",
    "    if featureDF.shape!= (0,0):\n",
    "        pickle_in = open(\"/home/penglab/FeaCal/ns.pickle\",\"rb\")\n",
    "        example_ = pickle.load(pickle_in)\n",
    "        ns_input= example_[0]\n",
    "\n",
    "        index_origin = ns_input.metadata.index.tolist()\n",
    "        ns_input.UMAP = nmt.UMAP_wrapper(featureDF, n_neighbors=100, min_dist=0.1, n_components=2, metric='euclidean',\n",
    "                                   PCA_first=True,n_PC=100)\n",
    "        index_after = ns_input.UMAP.index.tolist()\n",
    "        if len(index_after)>len(index_origin ):\n",
    "            index_update = [i for i in index_after if i in index_origin ]\n",
    "        else:\n",
    "            index_update = [i for i in index_origin if i in index_after ]\n",
    "        \n",
    "        ns_input.metadata = ns_input.metadata.loc[index_update,:]\n",
    "    cur_clusters = coclusterResult(Z_sample)\n",
    "    ns_input.metadata['Cluster'] = ['C' + str(i) for i in cur_clusters] \n",
    "    _ = ns_input.FeatureScatter([\"CellType\", \"Hemisphere\",\"Cluster\"], map=\"UMAP\")\n",
    "    return ns_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ztemp= bestCOCLUSTER(AP_hdbscanDF)\n",
    "generateNS(AP_hdbscanDF,ns,Ztemp,'/home/penglab/FeaCal/','AP_hdbscan_ns')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
