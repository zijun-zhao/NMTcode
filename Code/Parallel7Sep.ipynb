{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/penglab/anaconda3/lib/python3.7/site-packages/neuro_morpho_toolbox/\n",
      "Loading CCF Atlas data...\n",
      "Loading time: 0.87\n",
      "Loading CCF brain structure data...\n",
      "Loading time: 0.00\n",
      "Loading...\n",
      "number of files under swc_path: 1002\n",
      "100 loaded: 2.8s\n",
      "Invalid number of soma found: 0\n",
      "QC failed: AA0114.swc\n",
      "Invalid number of soma found: 0\n",
      "QC failed: AA0115.swc\n",
      "200 loaded: 2.5s\n",
      "300 loaded: 3.0s\n",
      "400 loaded: 2.9s\n",
      "Invalid number of soma found: 0\n",
      "QC failed: AA0472.swc\n",
      "500 loaded: 2.9s\n",
      "Invalid number of soma found: 0\n",
      "QC failed: AA0576.swc\n",
      "Invalid number of soma found: 0\n",
      "QC failed: AA0585.swc\n",
      "Invalid number of soma found: 0\n",
      "QC failed: AA0589.swc\n",
      "600 loaded: 2.5s\n",
      "Invalid number of soma found: 0\n",
      "QC failed: AA0639.swc\n",
      "Invalid number of soma found: 0\n",
      "QC failed: AA0670.swc\n",
      "Invalid number of soma found: 0\n",
      "QC failed: AA0672.swc\n",
      "700 loaded: 2.3s\n",
      "Invalid number of soma found: 0\n",
      "QC failed: AA0754.swc\n",
      "Invalid number of soma found: 0\n",
      "QC failed: AA0763.swc\n",
      "800 loaded: 2.8s\n",
      "900 loaded: 1.9s\n",
      "QC failed: AA0952.swc\n",
      "QC failed: AA0964.swc\n",
      "QC failed: AA0968.swc\n",
      "QC failed: AA0977.swc\n",
      "Finding soma locations...\n",
      "Getting projection features...\n",
      "Number of input neurons: 987\n",
      "Number of input features: 632\n",
      "Getting dendrite features...\n",
      "Number of input neurons: 987\n",
      "Number of input features: 316\n",
      "All values are zeros for the following cells:\n",
      "AA0411\n",
      "AA0931\n",
      "Getting metadata...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/penglab/anaconda3/lib/python3.7/site-packages/neuro_morpho_toolbox/neuron_features.py:337: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in true_divide\n",
      "\n",
      "/home/penglab/anaconda3/lib/python3.7/site-packages/neuro_morpho_toolbox/neuron_features.py:337: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "%matplotlib inline\n",
    "\n",
    "import neuro_morpho_toolbox as nmt\n",
    "ns = nmt.neuron_set('/home/penglab/Documents/Janelia_1000')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Using  soma location as features\n",
    "SETTING CLUSTER NUMBER FROM 8 TO 40\n",
    "\n",
    "### Hierarchy Clustering\n",
    "For Hierarchy method\n",
    "* the most suitable parameter is {'L_method': 'complete', 'L_metric': 'braycurtis', 'criterionH': 'distance', 'depth': 2, 'R': None, 't': 0.9, 'optimal_ordering': False}\n",
    "* the ARI is 0.3268728951610669\n",
    "* The setting cluster number's limit is satisfied, the final number of cluster is 9\n",
    "\n",
    "### Kmeans Clustering\n",
    "For Kmeans method\n",
    "* the most suitable parameter is {'n_clusters': 10, 'init': 'random', 'n_init': 7, 'max_iter': 300, 'tol': 0.0001, 'precompute_distances': 'auto', 'verbose': 0, 'random_state': None, 'copy_x': True, 'n_jobs': None, 'algorithm': 'auto'}\n",
    "* the ARI is 0.27863577689094327\n",
    "* The setting cluster number's limit is satisfied, the final number of cluster is 10\n",
    "\n",
    "### DBSCAN Clustering\n",
    "For DBSCAN method\n",
    "* the most suitable parameter is {'eps': 0.9, 'min_samples': 5, 'metric': 'euclidean', 'metric_params': None, 'algorithm': 'auto', 'leaf_size': 30, 'p': None, 'n_jobs': None}\n",
    "* the ARI is 0.2630886481583674\n",
    "* The setting cluster number's limit is satisfied, the final number of cluster is 29\n",
    "\n",
    "### HDBSCAN Clustering\n",
    "For HDBSCAN method, \n",
    "* the most suitable parameter is {'min_cluster_size': 5, 'metric': 'euclidean', 'alpha': 0.9, 'min_samples': 9, 'p': 2, 'algorithm': 'generic', 'leaf_size': 40, 'approx_min_span_tree': True, 'gen_min_span_tree': False, 'core_dist_n_jobs': 4, 'cluster_selection_method': 'eom', 'allow_single_cluster': False, 'prediction_data': False, 'match_reference_implementation': False}\n",
    "* the ARI is 0.26100195542904575\n",
    "* The setting cluster number's limit is satisfied, the final number of cluster is 34\n",
    "\n",
    "### SNN Clustering\n",
    "For SNN_community method\n",
    "* the most suitable parameter is {'knn': 5, 'metric': 'minkowski', 'method': 'FastGreedy'}\n",
    "* the ARI is 0.13064660013981225\n",
    "* The setting cluster number's limit is satisfied, the final number of cluster is 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_hier =  {'L_method': 'centroid', 'L_metric': 'euclidean', 'criterionH': 'distance', 'depth': 2, 'R': None,\n",
    "             't': 0.9, 'optimal_ordering': False}\n",
    "par_kmeans = {'n_clusters': 14, 'init': 'random', 'n_init': 20, 'max_iter': 300, 'tol': 0.0001, \n",
    "              'precompute_distances': 'auto', 'verbose': 0, 'random_state': None, 'copy_x': True, 'n_jobs': None,\n",
    "              'algorithm': 'full'}\n",
    "\n",
    "par_dbscan = {'eps': 0.215, 'min_samples': 5, 'metric': 'euclidean', 'metric_params': None, 'algorithm': 'auto', \n",
    "              'leaf_size': 30, 'p': None, 'n_jobs': None}\n",
    "\n",
    "par_hdbscan = {'min_cluster_size': 5, 'metric': 'euclidean', 'alpha': 0.8, 'min_samples': 7, 'p': 2, 'algorithm': \n",
    "               'generic', 'leaf_size': 40, 'approx_min_span_tree': True, 'gen_min_span_tree': False, \n",
    "               'core_dist_n_jobs': 4, 'cluster_selection_method': 'eom', 'allow_single_cluster': False, \n",
    "               'prediction_data': False, 'match_reference_implementation': False}\n",
    "\n",
    "par_snn = {'knn':5,'metric':'minkowski','method':'FastGreedy'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(inputUMAP,method='SNN_community',karg_dict={'knn':5, 'metric':'minkowski','method':'FastGreedy'}):\n",
    "    methods_allowed = ['SNN_community', 'Hierarchy', 'Kmeans', 'DBSCAN', 'HDBSCAN']\n",
    "    assert method in methods_allowed, \"Please set 'method' as one of the following: 'SNN_community', 'Hierarchy', 'Kmeans', 'DBSCAN', 'HDBSCAN'\"\n",
    "    selectedUMAP = inputUMAP.copy()\n",
    "    if method=='SNN_community':\n",
    "        #print('Result of SNN_community')\n",
    "        if 'knn' in karg_dict.keys():\n",
    "            knn = karg_dict['knn']\n",
    "        else:\n",
    "            knn = 5\n",
    "        if 'metric' in karg_dict.keys():\n",
    "            metric = karg_dict['metric']\n",
    "        else:\n",
    "            metric = 'minkowski'\n",
    "        if 'method' in karg_dict.keys():\n",
    "            community_method = karg_dict['method']\n",
    "        else:\n",
    "            community_method = 'FastGreedy'\n",
    "        cur_clusters = nmt.get_clusters_SNN_community(selectedUMAP, knn=knn, metric=metric, method=community_method)\n",
    "        \n",
    "\n",
    "    #karg_dict={'L_method':'single','L_metric':'euclidean'.'t':0.9,'criterionH':'inconsistent', depth=2, R=None, monocrit=None}\n",
    "    if method =='Hierarchy':\n",
    "        #print('Result of Hierarchy CLustering')\n",
    "        cur_clusters = nmt.get_clusters_Hierarchy_clustering(selectedUMAP, karg_dict)\n",
    "\n",
    "\n",
    "    if method =='Kmeans':\n",
    "        #print('Result of Kmeans')\n",
    "        cur_clusters = nmt.get_clusters_kmeans_clustering(selectedUMAP, karg_dict)\n",
    "\n",
    "    if method =='DBSCAN':\n",
    "        #print('Result of DBSCAN')\n",
    "        cur_clusters = nmt.get_clusters_dbscan_clustering(selectedUMAP, karg_dict)\n",
    "\n",
    "    if method =='HDBSCAN':\n",
    "        #print('Result of HDBSCAN')\n",
    "        cur_clusters = nmt.get_clusters_hdbscan_clustering(selectedUMAP, karg_dict)\n",
    "    selectedUMAP.loc[:,'Cluster'] = ['C' + str(i) for i in cur_clusters]\n",
    "    return selectedUMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function freq_Matrix(fre_M, cluster_method,para_test)\n",
    "* **fre_M** is the square matrix recording the number of co-clustering\n",
    "* **cluster_method** can be 'Hierarchy','Kmeans', 'DBSCAN','HDBSCAN','SNN_community'\n",
    "* **para_test** is the input parameter dictionary for the cluster method\n",
    "* **iternum** is the number of iteration to generate the coclustering matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import ast\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import time\n",
    "def fre_Matrix(fre_M, cluster_method,para_test):\n",
    "    umapDF = ns.UMAP.iloc[random.sample(range(0,ns.UMAP.shape[0]), int(ns.UMAP.shape[0]*0.95)),:].copy()\n",
    "    resultDF = get_clusters(umapDF.copy(),method =cluster_method,karg_dict = para_test)\n",
    "    Crange, Ccounts = np.unique(resultDF[\"Cluster\"], return_counts = True)\n",
    "    for iter_C in Crange:\n",
    "        selected_row = resultDF[resultDF[\"Cluster\"]==iter_C]\n",
    "        Clist = selected_row.index.tolist()\n",
    "        for sample_row in Clist:\n",
    "            for sample_col in Clist:\n",
    "                fre_M.loc[sample_row,sample_col] =  fre_M.loc[sample_row,sample_col]+1\n",
    "    return fre_M.values\n",
    "def para_cocluster(cluster_method,para_test,corenum, run_num,ns_input):\n",
    "    start = time.perf_counter ()\n",
    "    start=time.time()\n",
    "    cores = corenum#multiprocessing.cpu_count()\n",
    "    pool = multiprocessing.Pool(processes=cores)\n",
    "    fre_M_t = pd.DataFrame(index = ns_input.UMAP.index, columns =ns_input.UMAP.index)\n",
    "    fre_M_t [fre_M_t.isnull()]=0\n",
    "    pool_list=[]\n",
    "    result_list=[]\n",
    "    for i in range(run_num):\n",
    "        pool_list.append(pool.apply_async(fre_Matrix, (fre_M_t, cluster_method, para_test)))\n",
    "        # 这里不能 get， 会阻塞进程\n",
    "\n",
    "    #pool.apply_async之后的语句都是阻塞执行的，\n",
    "    #调用 result.get() 会等待上一个任务执行完之后才会分配下一个任务。\n",
    "    #事实上，获取返回值的过程最好放在进程池回收之后进行，避免阻塞后面的语句。\n",
    "    result_list=[xx.get() for xx in pool_list]\n",
    "    print(sum([xx for xx in  result_list]))\n",
    "    # 最后我们使用一下语句回收进程池:\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    elapsed = (time.time() - start)\n",
    "    print('Time needed to run Hierarchy is '+ str(elapsed))\n",
    "    return sum([xx for xx in  result_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ns.ReduceDimUMAP(feature_set=\"projection_features\", n_neighbors=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For axon projection features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AP_hier = para_cocluster('Hierarchy', par_hier,30, 5000,ns)\n",
    "AP_kmeans = para_cocluster('Kmeans', par_kmeans,30, 5000,ns)\n",
    "AP_dbscan = para_cocluster('DBSCAN', par_dbscan,30, 5000,ns)\n",
    "AP_hdbscan = para_cocluster('HDBSCAN', par_hdbscan,30, 5000,ns)\n",
    "AP_snn = para_cocluster('SNN_community', par_snn,30, 5000,ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AP_hierDF = pd.DataFrame(data=AP_hier, index=ns.UMAP.index, columns=ns.UMAP.index)\n",
    "AP_hierDF.to_excel('/home/penglab/FeaCal/dataSource/axonProj/AP_hierDF.xlsx')\n",
    "AP_kmeansDF = pd.DataFrame(data=AP_kmeans, index=ns.UMAP.index, columns=ns.UMAP.index)\n",
    "AP_kmeansDF.to_excel('/home/penglab/FeaCal/dataSource/axonProj/AP_kmeansDF.xlsx')\n",
    "AP_dbscanDF = pd.DataFrame(data=AP_dbscan, index=ns.UMAP.index, columns=ns.UMAP.index)\n",
    "AP_dbscanDF.to_excel('/home/penglab/FeaCal/dataSource/axonProj/AP_dbscanDF.xlsx')\n",
    "AP_hdbscanDF = pd.DataFrame(data=AP_hdbscan, index=ns.UMAP.index, columns=ns.UMAP.index)\n",
    "AP_hdbscanDF.to_excel('/home/penglab/FeaCal/dataSource/axonProj/AP_hdbscanDF.xlsx')\n",
    "AP_snnDF = pd.DataFrame(data=AP_snn, index=ns.UMAP.index, columns=ns.UMAP.index)\n",
    "AP_snnDF.to_excel('/home/penglab/FeaCal/dataSource/axonProj/AP_snnDF.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soma features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = nmt.neuron_set('/home/penglab/Documents/Janelia_1000')\n",
    "_ = ns.ReduceDimUMAP(feature_set=\"soma_features\", n_neighbors=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_hier2 =  {'L_method': 'complete', 'L_metric': 'braycurtis', 'criterionH': 'distance', 'depth': 2, 'R': None, \n",
    "             't': 0.9, 'optimal_ordering': False} \n",
    "par_kmeans2 = {'n_clusters': 10, 'init': 'random', 'n_init': 7, 'max_iter': 300, 'tol': 0.0001, \n",
    "              'precompute_distances': 'auto', 'verbose': 0, 'random_state': None, 'copy_x': True, 'n_jobs': None, \n",
    "              'algorithm': 'auto'} \n",
    "\n",
    "par_dbscan2 = {'eps': 0.9, 'min_samples': 5, 'metric': 'euclidean', 'metric_params': None, 'algorithm': 'auto', \n",
    "              'leaf_size': 30, 'p': None, 'n_jobs': None}\n",
    "\n",
    "par_hdbscan2 =  {'min_cluster_size': 5, 'metric': 'euclidean', 'alpha': 0.9, 'min_samples': 9, 'p': 2, 'algorithm': \n",
    "                'generic', 'leaf_size': 40, 'approx_min_span_tree': True, 'gen_min_span_tree': False, \n",
    "                'core_dist_n_jobs': 4, 'cluster_selection_method': 'eom', 'allow_single_cluster': False, \n",
    "                'prediction_data': False, 'match_reference_implementation': False} \n",
    "\n",
    "par_snn2 = {'knn':5,'metric':'minkowski','method':'FastGreedy'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SL_hier = para_cocluster('Hierarchy', par_hier2,30, 5000,ns)\n",
    "SL_kmeans = para_cocluster('Kmeans', par_kmeans2,30, 5000,ns)\n",
    "SL_dbscan = para_cocluster('DBSCAN', par_dbscan2,30, 5000,ns)\n",
    "SL_hdbscan = para_cocluster('HDBSCAN', par_hdbscan2,30, 5000,ns)\n",
    "SL_snn = para_cocluster('SNN_community', par_snn2,30, 5000,ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SL_hierDF = pd.DataFrame(data=SL_hier, index=ns.UMAP.index, columns=ns.UMAP.index)\n",
    "SL_hierDF.to_excel('/home/penglab/FeaCal/dataSource/axonProj/SL_hierDF.xlsx')\n",
    "SL_kmeansDF = pd.DataFrame(data=SL_kmeans, index=ns.UMAP.index, columns=ns.UMAP.index)\n",
    "SL_kmeansDF.to_excel('/home/penglab/FeaCal/dataSource/axonProj/SL_kmeansDF.xlsx')\n",
    "SL_dbscanDF = pd.DataFrame(data=SL_dbscan, index=ns.UMAP.index, columns=ns.UMAP.index)\n",
    "SL_dbscanDF.to_excel('/home/penglab/FeaCal/dataSource/axonProj/SL_dbscanDF.xlsx')\n",
    "SL_hdbscanDF = pd.DataFrame(data=SL_hdbscan, index=ns.UMAP.index, columns=ns.UMAP.index)\n",
    "SL_hdbscanDF.to_excel('/home/penglab/FeaCal/dataSource/axonProj/SL_hdbscanDF.xlsx')\n",
    "SL_snnDF = pd.DataFrame(data=SL_snn, index=ns.UMAP.index, columns=ns.UMAP.index)\n",
    "SL_snnDF.to_excel('/home/penglab/FeaCal/dataSource/axonProj/SL_snnDF.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
