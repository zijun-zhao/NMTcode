{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from shutil import copyfile\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import colorlover as cl\n",
    "\n",
    "\n",
    "import neuro_morpho_toolbox as nmt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "pickle_in = open(\"/home/penglab/FeaCal/all_ns.pickle\",\"rb\")\n",
    "[ns] = pickle.load(pickle_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = ns.features['soma_features'].region\n",
    "ct = sf[\"Region\"].value_counts().sort_values(ascending=False)\n",
    "_ = sns.countplot(y=\"Region\", \n",
    "                  data=sf.loc[sf.Region.isin(ct.index[ct>10])],\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ns.ReduceDimUMAP(feature_set=\"projection_features\", n_neighbors=100)\n",
    "_ = ns.FeatureScatter([\"CellType\", \"Hemisphere\"], map=\"UMAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_to_name(self, region_ID):\n",
    "        # region_name can be either Abbreviation (checked first) or description\n",
    "        if region_ID in self.level.index.tolist():\n",
    "            return self.level.loc[region_ID,'Abbreviation']\n",
    "        else:\n",
    "            print(\"Cannot find any regions with ID %s.\" % region_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "def Contour_Matrix(M_ori, M_contour, x_in, y_in, z_in):\n",
    "    assert np.sum(M_contour)== 0\n",
    "    assert M_contour.shape == M_ori.shape\n",
    "    centerV =  M_ori[x_in,y_in, z_in]  \n",
    "    assert M_ori.shape == nmt.annotation.array.shape\n",
    "    x1 = M_ori[max(0,x_in-1),y_in,z_in]            # -1,0,0\n",
    "    x2 = M_ori[min(M_ori.shape[0]-1,x_in+1),y_in,z_in]#+1,0,0\n",
    "    y1 = M_ori[x_in,max(0,y_in-1),z_in]# 0,-1,0\n",
    "    y2 = M_ori[x_in,min(M_ori.shape[1]-1,y_in+1),z_in] # 0,+1,0\n",
    "    z1 = M_ori[x_in,y_in,max(0,z_in-1)]# 0,0,-1\n",
    "    z2 = M_ori[x_in,y_in,min(M_ori.shape[2]-1,z_in+1)]# 0,0,+1\n",
    "    if not all(v == centerV for v in [x1,x2,y1,y2,z1,z2]):\n",
    "        M_contour[x_in,y_in,z_in] = centerV\n",
    "    return M_contour.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "corenum = 20\n",
    "Regtype, Regcount = np.unique(nmt.annotation.array, return_counts=True) \n",
    "\n",
    "ContourDic = {}\n",
    "\n",
    "start = time.perf_counter ()\n",
    "start=time.time()\n",
    "iterReg = 0\n",
    "cores = corenum#multiprocessing.cpu_count()\n",
    "pool = multiprocessing.Pool(processes=cores)\n",
    "# array where the index is not 0\n",
    "M_region = nmt.annotation.array.copy()\n",
    "M_region = M_region!= iterReg  # a np.ndarray\n",
    "M_origin = nmt.annotation.array.copy()\n",
    "co_1,co_2,co_3 = np.where(nmt.annotation.array != 0) \n",
    "borderM = nmt.annotation.array.copy()\n",
    "borderM[:,:,:] = 0\n",
    "pool_list=[]\n",
    "result_list=[]\n",
    "for iter_COR in range(len(co_1)):\n",
    "    pool_list.append(pool.apply_async(Contour_Matrix, (M_origin,borderM, co_1[iter_COR], co_2[iter_COR], co_3[iter_COR])))\n",
    "    print('Loading progess: %.6f' % (iter_COR/len(co_1)))\n",
    "result_list=[xx.get() for xx in pool_list]\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "elapsed = (time.time() - start)\n",
    "print('Time needed to run the whole matrix is '+ str(elapsed))\n",
    "ContourDic['ALL']= sum([xx for xx in  result_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = np.zeros(nmt.annotation.array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "def Contour_Matrix(M_orix_in, y_in, z_in):\n",
    "    M_contour = M_ori.copy()\n",
    "    M_contour[:,:,:] = 0\n",
    "    centerV =  M_ori[x_in,y_in, z_in]  \n",
    "    assert M_ori.shape == nmt.annotation.array.shape\n",
    "    x1 = M_ori[max(0,x_in-1),y_in,z_in]            # -1,0,0\n",
    "    x2 = M_ori[min(M_ori.shape[0]-1,x_in+1),y_in,z_in]#+1,0,0\n",
    "    y1 = M_ori[x_in,max(0,y_in-1),z_in]# 0,-1,0\n",
    "    y2 = M_ori[x_in,min(M_ori.shape[1]-1,y_in+1),z_in] # 0,+1,0\n",
    "    z1 = M_ori[x_in,y_in,max(0,z_in-1)]# 0,0,-1\n",
    "    z2 = M_ori[x_in,y_in,min(M_ori.shape[2]-1,z_in+1)]# 0,0,+1\n",
    "    if not all(v == centerV for v in [x1,x2,y1,y2,z1,z2]):\n",
    "        M_contour[x_in,y_in,z_in] = 1\n",
    "    return [x_in,y_in,z_in]\n",
    "\n",
    "corenum = 12\n",
    "Regtype, Regcount = np.unique(nmt.annotation.array, return_counts=True) \n",
    "\n",
    "ContourDic = {}\n",
    "\n",
    "start = time.perf_counter ()\n",
    "start=time.time()\n",
    "iterReg = 0\n",
    "cores = corenum#multiprocessing.cpu_count()\n",
    "pool = multiprocessing.Pool(processes=cores)\n",
    "# array where the index is not 0\n",
    "M_region = nmt.annotation.array.copy()\n",
    "M_region = M_region!= iterReg  # a np.ndarray\n",
    "M_origin = nmt.annotation.array.copy()\n",
    "co_1,co_2,co_3 = np.where(nmt.annotation.array != 0) \n",
    "\n",
    "pool_list=[]\n",
    "result_list=[]\n",
    "for iter_COR in range(len(co_1)):\n",
    "    pool_list.append(pool.apply_async(Contour_Matrix, (M_origin, co_1[iter_COR], \n",
    "                                                       co_2[iter_COR], co_3[iter_COR])))\n",
    "    print('Loading progess: %.6f' % (iter_COR/len(co_1)))\n",
    "result_list=[xx.get() for xx in pool_list]\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "elapsed = (time.time() - start)\n",
    "print('Time needed to run the whole matrix is '+ str(elapsed))\n",
    "#ContourDic['ALL']= ([xx for xx in  result_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_origin[1,1,1]=5\n",
    "M_ttt = M_origin.copy()\n",
    "M_ttt[1,1,1]=14\n",
    "M_origin[1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "corenum = 12\n",
    "Regtype, Regcount = np.unique(nmt.annotation.array, return_counts=True) \n",
    "\n",
    "ContourDic = {}\n",
    "\n",
    "start = time.perf_counter ()\n",
    "start=time.time()\n",
    "iterReg = 0\n",
    "cores = corenum#multiprocessing.cpu_count()\n",
    "pool = multiprocessing.Pool(processes=cores)\n",
    "# array where the index is not 0\n",
    "M_region = nmt.annotation.array.copy()\n",
    "M_region = M_region!= iterReg  # a np.ndarray\n",
    "M_origin = nmt.annotation.array.copy()\n",
    "co_1,co_2,co_3 = np.where(nmt.annotation.array != 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_1,co_2,co_3 = np.where(nmt.annotation.array != 0)\n",
    "len(co_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "hier_resultDF = pd.read_excel('/home/penglab/NMTcode/hier_resultDF.xlsx')\n",
    "kmeans_resultDF = pd.read_excel('/home/penglab/NMTcode/kmeans_resultDF.xlsx')\n",
    "dbscan_resultDF = pd.read_excel('/home/penglab/NMTcode/dbscan_resultDF.xlsx')\n",
    "hdbscan_resultDF = pd.read_excel('/home/penglab/NMTcode/hdbscan_resultDF.xlsx')\n",
    "snn_resultDF= pd.read_excel('/home/penglab/NMTcode/snn_resultDF.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.arange(9).reshape((3, 3))\n",
    "x2 = np.arange(9).reshape((3, 3))\n",
    "np.dot(x1,x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6- nearest array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_mask = nmt.annotation.array.copy()\n",
    "np.sum(M_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_1[iter_pixel]+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_mask[co_1[iter_pixel]-1:co_1[iter_pixel]+2,co_2[iter_pixel]-1:co_2[iter_pixel]+2,co_3[iter_pixel]-1:co_3[iter_pixel]+2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        M_mask = nmt.annotation.array.copy()\n",
    "        M_mask[:,:,:] = 0\n",
    "        M_mask[max(0,55-1):min(M_mask.shape[0],55+1),\n",
    "                   max(0,55-1):min(M_mask.shape[1],55+1),\n",
    "               max(0,55-1):min(M_mask.shape[2],55+1)] = 1\n",
    "        print(np.sum(np.sum(M_mask)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Regtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Regtype, Regcount = np.unique(nmt.annotation.array, return_counts=True) \n",
    "\n",
    "ContourDic = {}\n",
    "i_p=0\n",
    "for iterReg in Regtype[1:]:\n",
    "    M_region = nmt.annotation.array == iterReg  # a np.ndarray\n",
    "    co_1,co_2,co_3 = np.where(nmt.annotation.array == iterReg) \n",
    "    borderM = nmt.annotation.array.copy()\n",
    "    borderM[:,:,:] = 0\n",
    "    for iter_pixel in range(len(co_1)):\n",
    "        M_mask = nmt.annotation.array.copy()\n",
    "        M_mask[:,:,:] = 0\n",
    "        M_mask[max(0,co_1[iter_pixel]-1),co_2[iter_pixel],co_3[iter_pixel]]=1\n",
    "        M_mask[min(nmt.annotation.array.shape[0]-1,co_1[iter_pixel]+1),co_2[iter_pixel],co_3[iter_pixel]]=1\n",
    "        M_mask[co_1[iter_pixel],max(0,co_2[iter_pixel]-1),co_3[iter_pixel]]=1\n",
    "        M_mask[co_1[iter_pixel],min(nmt.annotation.array.shape[1]-1,co_2[iter_pixel]+1),co_3[iter_pixel]]=1\n",
    "        M_mask[co_1[iter_pixel],co_2[iter_pixel],max(0,co_3[iter_pixel]-1)]=1\n",
    "        M_mask[co_1[iter_pixel],co_2[iter_pixel],min(nmt.annotation.array.shape[2]-1,co_3[iter_pixel]+1)]=1\n",
    "        assert M_mask.shape == nmt.annotation.array.shape\n",
    "        if np.sum(np.sum(np.multiply(M_region==0,M_mask)))>0:\n",
    "            borderM[co_1[iter_pixel],co_2[iter_pixel],co_2[iter_pixel]]=1\n",
    "        print('Loading progess inside region ' +str(iterReg)+' : %.5f' % (iter_pixel/len(co_1)))\n",
    "    ContourDic[iterReg] = borderM\n",
    "    print('Loading progess for all brain regions: %.2f' % (i_p/len(Regtype)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for iter_pixel in range(len(co_1)):\n",
    "        M_mask = nmt.annotation.array.copy()\n",
    "        M_mask[:,:,:] = 0\n",
    "        M_mask[max(0,co_1[iter_pixel]-1),co_2[iter_pixel],co_3[iter_pixel]]=1\n",
    "        M_mask[min(nmt.annotation.array.shape[0]-1,co_1[iter_pixel]+1),co_2[iter_pixel],co_3[iter_pixel]]=1\n",
    "        M_mask[co_1[iter_pixel],max(0,co_2[iter_pixel]-1),co_3[iter_pixel]]=1\n",
    "        M_mask[co_1[iter_pixel],min(nmt.annotation.array.shape[1]-1,co_2[iter_pixel]+1),co_3[iter_pixel]]=1\n",
    "        M_mask[co_1[iter_pixel],co_2[iter_pixel],max(0,co_3[iter_pixel]-1)]=1\n",
    "        M_mask[co_1[iter_pixel],co_2[iter_pixel],min(nmt.annotation.array.shape[2]-1,co_3[iter_pixel]+1)]=1\n",
    "        assert M_mask.shape == nmt.annotation.array.shape\n",
    "        if np.sum(np.sum(np.multiply(M_reg==0,M_mask)))>0:\n",
    "            M_contour[co_1[iter_pixel],co_2[iter_pixel],co_2[iter_pixel]]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "def Contour_Matrix(M_reg, M_contour, co_1_iter, co_2_iter, co_3_iter):\n",
    "    M_contour = nmt.annotation.array.copy()\n",
    "    M_contour[:,:,:] = 0\n",
    "    for iter_pixel in range(len(co_1)):\n",
    "        M_mask = nmt.annotation.array.copy()\n",
    "        M_mask[:,:,:] = 0\n",
    "        M_mask[max(0,co_1[iter_pixel]-1),co_2[iter_pixel],co_3[iter_pixel]]=1\n",
    "        M_mask[min(nmt.annotation.array.shape[0]-1,co_1[iter_pixel]+1),co_2[iter_pixel],co_3[iter_pixel]]=1\n",
    "        M_mask[co_1[iter_pixel],max(0,co_2[iter_pixel]-1),co_3[iter_pixel]]=1\n",
    "        M_mask[co_1[iter_pixel],min(nmt.annotation.array.shape[1]-1,co_2[iter_pixel]+1),co_3[iter_pixel]]=1\n",
    "        M_mask[co_1[iter_pixel],co_2[iter_pixel],max(0,co_3[iter_pixel]-1)]=1\n",
    "        M_mask[co_1[iter_pixel],co_2[iter_pixel],min(nmt.annotation.array.shape[2]-1,co_3[iter_pixel]+1)]=1\n",
    "        assert M_mask.shape == nmt.annotation.array.shape\n",
    "        if np.sum(np.sum(np.multiply(M_reg==0,M_mask)))>0:\n",
    "            M_contour[co_1[iter_pixel],co_2[iter_pixel],co_2[iter_pixel]]=1\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "def Contour_Matrix(M_reg, M_contour, co_1_iter, co_2_iter, co_3_iter):\n",
    "    M_contour = nmt.annotation.array.copy()\n",
    "    M_contour[:,:,:] = 0\n",
    "    for iter_pixel in range(len(co_1_iter)):\n",
    "        print((co_1[iter_pixel],co_2[iter_pixel],co_2[iter_pixel]))\n",
    "        M_mask = nmt.annotation.array.copy()\n",
    "        M_mask[:,:,:] = 0\n",
    "        assert M_mask.shape == nmt.annotation.array.shape\n",
    "        M_mask[max(0,co_1[iter_pixel]-1),co_2[iter_pixel],co_3[iter_pixel]]=1\n",
    "        M_mask[min(nmt.annotation.array.shape[0]-1,co_1[iter_pixel]+1),co_2[iter_pixel],co_3[iter_pixel]]=1\n",
    "        M_mask[co_1[iter_pixel],max(0,co_2[iter_pixel]-1),co_3[iter_pixel]]=1\n",
    "        M_mask[co_1[iter_pixel],min(nmt.annotation.array.shape[1]-1,co_2[iter_pixel]+1),co_3[iter_pixel]]=1\n",
    "        M_mask[co_1[iter_pixel],co_2[iter_pixel],max(0,co_3[iter_pixel]-1)]=1\n",
    "        M_mask[co_1[iter_pixel],co_2[iter_pixel],min(nmt.annotation.array.shape[2]-1,co_3[iter_pixel]+1)]=1\n",
    "        assert M_mask.shape == nmt.annotation.array.shape\n",
    "        if np.sum(np.sum(np.multiply(M_reg==0,M_mask)))>0:\n",
    "            M_contour[co_1[iter_pixel],co_2[iter_pixel],co_2[iter_pixel]]=1\n",
    "            #print((co_1[iter_pixel],co_2[iter_pixel],co_2[iter_pixel]))\n",
    "    return \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corenum = 12\n",
    "Regtype, Regcount = np.unique(nmt.annotation.array, return_counts=True) \n",
    "\n",
    "ContourDic = {}\n",
    "\n",
    "for iterReg in Regtype[::-1]:\n",
    "    start = time.perf_counter ()\n",
    "    start=time.time()\n",
    "    cores = corenum#multiprocessing.cpu_count()\n",
    "    pool = multiprocessing.Pool(processes=cores)\n",
    "    M_region = nmt.annotation.array != iterReg  # a np.ndarray\n",
    "    co_1,co_2,co_3 = np.where(nmt.annotation.array == iterReg) \n",
    "    borderM = nmt.annotation.array.copy()\n",
    "    borderM[:,:,:] = 0\n",
    "    pool_list=[]\n",
    "    result_list=[]\n",
    "    pool_list.append(pool.apply_async(Contour_Matrix, (M_region,borderM, co_1, co_2, co_3)))\n",
    "        # 这里不能 get， 会阻塞进程\n",
    "\n",
    "    #pool.apply_async之后的语句都是阻塞执行的，\n",
    "    #调用 result.get() 会等待上一个任务执行完之后才会分配下一个任务。\n",
    "    #事实上，获取返回值的过程最好放在进程池回收之后进行，避免阻塞后面的语句。\n",
    "    result_list=[xx.get() for xx in pool_list]\n",
    "    print(sum([xx for xx in  result_list]))\n",
    "    # 最后我们使用一下语句回收进程池:\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    elapsed = (time.time() - start)\n",
    "    print('Time needed to run region '+ str(iterReg)+ 'is '+ str(elapsed))\n",
    "    ContourDic[iterReg]= sum([xx for xx in  result_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import multiprocessing\n",
    "def test1(borderM, x,y,z):\n",
    "    print(len(x))\n",
    "    borderM[x,y,z]=1\n",
    "    return(x*x,y*y,z*z)\n",
    "\n",
    "corenum = 12\n",
    "\n",
    "i_p=0\n",
    "for iterReg in Regtype[1:]:\n",
    "    start = time.perf_counter ()\n",
    "    start=time.time()\n",
    "    cores = corenum#multiprocessing.cpu_count()\n",
    "    pool = multiprocessing.Pool(processes=cores)\n",
    "    M_region = nmt.annotation.array == iterReg  # a np.ndarray\n",
    "    co_1,co_2,co_3 = np.where(nmt.annotation.array == iterReg) \n",
    "    co_1 = (np.arange(10))\n",
    "    co_2 = (np.arange(5))\n",
    "    co_3 = (np.arange(6))\n",
    "    borderM = nmt.annotation.array.copy()\n",
    "    borderM[:,:,:] = 0\n",
    "    pool_list=[]\n",
    "    result_list=[]\n",
    "    pool_list.append(pool.apply_async(test1, (borderM,co_1, co_2, co_3)))\n",
    "        # 这里不能 get， 会阻塞进程\n",
    "\n",
    "    #pool.apply_async之后的语句都是阻塞执行的，\n",
    "    #调用 result.get() 会等待上一个任务执行完之后才会分配下一个任务。\n",
    "    #事实上，获取返回值的过程最好放在进程池回收之后进行，避免阻塞后面的语句。\n",
    "    result_list=[xx.get() for xx in pool_list]\n",
    "    print([xx for xx in  result_list])\n",
    "    # 最后我们使用一下语句回收进程池:\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    elapsed = (time.time() - start)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "somaDF = ns.features['soma_features'].raw_data.copy()\n",
    "for iter_idx in somaDF.index.tolist():\n",
    "    ID_temp = nmt.annotation.array[somaDF.loc[iter_idx,'x'],somaDF.loc[iter_idx,'y'],somaDF.loc[iter_idx,'z']]\n",
    "    M_region = nmt.annotation.array == ID_temp  # a np.ndarray\n",
    "    M_mask = nmt.annotation.array.copy()\n",
    "    M_mask[:,:,:] = 0\n",
    "    mul_i = 1\n",
    "    while np.sum(np.sum(np.multiply(M_region, M_mask)))==0:\n",
    "        x_range_1 = somaDF.loc[iter_idx,'x']-5* mul_i\n",
    "        x_range_2 = somaDF.loc[iter_idx,'x']+5* mul_i\n",
    "        y_range_1 = somaDF.loc[iter_idx,'y']-5* mul_i\n",
    "        y_range_2 = somaDF.loc[iter_idx,'y']+5* mul_i    \n",
    "        z_range_1 = somaDF.loc[iter_idx,'z']-5* mul_i\n",
    "        z_range_2 = somaDF.loc[iter_idx,'z']+5* mul_i  \n",
    "        M_mask[x_range_1:x_range_2,:,:] = 0\n",
    "        mul_i = mul_i +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape\n",
    "print('Shape: ', nmt.annotation.array.shape)\n",
    "\n",
    "# dtype\n",
    "print('Datatype: ', nmt.annotation.array.dtype)\n",
    "\n",
    "# size\n",
    "print('Size: ', nmt.annotation.array.size)\n",
    "\n",
    "# ndim\n",
    "print('Num Dimensions: ', nmt.annotation.array.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns.features['soma_features'].raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "%matplotlib notebook\n",
    "import mpld3\n",
    "mpld3.enable_notebook()\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "co_1,co_2,co_3 = np.where(nmt.annotation.array==9)\n",
    "co_1 = co_1[co_3>=nmt.annotation.size['z']//4]\n",
    "co_2 = co_2[co_3>=nmt.annotation.size['z']//4]\n",
    "co_3 = co_3[co_3>=nmt.annotation.size['z']//4]\n",
    "co_1 = co_1[co_3<=nmt.annotation.size['z']//2]\n",
    "co_2 = co_2[co_3<=nmt.annotation.size['z']//2]\n",
    "co_3 = co_3[co_3<=nmt.annotation.size['z']//2]\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(co_1, co_2, co_3, c='r', marker='o')\n",
    "\n",
    "\n",
    "co_4,co_5,co_6 = np.where(nmt.annotation.array==0)\n",
    "\n",
    "#choose_4 = np.sum((nmt.annotation.size['z']//4<=co_4)&(co_4<=nmt.annotation.size['z']//4))\n",
    "#choose_5 = np.sum((min(co_2)<=co_5)&(co_5<=max(co_2)))\n",
    "#choose_6 = np.sum((min(co_3)<=co_6)&(co_6<=max(co_3)))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "ax.scatter(co_4[(nmt.annotation.size['z']//4<=co_4)&(co_4<=nmt.annotation.size['z']//2)], co_5[(nmt.annotation.size['z']//4<=co_4)&(co_4<=nmt.annotation.size['z']//2)], co_6[(nmt.annotation.size['z']//4<=co_4)&(co_4<=nmt.annotation.size['z']//2)], c='g', marker='o')\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(co_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_1,co_2,co_3 = np.where(nmt.annotation.array==9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list9 = list(zip(co_1,co_2,co_3))\n",
    "list9[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_1,co_2,co_3 = np.where(nmt.annotation.array==9)\n",
    "co_1 = co_1[co_3<=nmt.annotation.size['z']//2]\n",
    "co_2 = co_2[co_3<=nmt.annotation.size['z']//2]\n",
    "co_3 = co_3[co_3<=nmt.annotation.size['z']//2]\n",
    "ls_9x = []\n",
    "ls_9y = []\n",
    "ls_9z = []\n",
    "ls_other= []\n",
    "for x,y,z in zip(co_1,co_2,co_3):\n",
    "    for i in range(-1,2):\n",
    "        for j in range(-1,2):\n",
    "            for k in range(-1,2):\n",
    "                if nmt.annotation.array[x+i,y+j,z+k] !=9:\n",
    "                    print(nmt.annotation.array[x+i,y+j,z+k])\n",
    "                    ls_9x.append(x)\n",
    "                    ls_9y.append(y)\n",
    "                    ls_9z.append(z)\n",
    "                    ls_other.append(nmt.annotation.array[x+i,y+j,z+k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(ls_9x, ls_9y, ls_9z, c='g', marker='o')\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDtype, IDcount = np.unique(ls_other, return_counts=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(co_1,co_2,co_3))\n",
    "ls_9 = []\n",
    "ls_0 = []\n",
    "\n",
    "for i in range(-1,2):\n",
    "    for j in range(-1,2):\n",
    "        for k in range(-1,2):\n",
    "            if nmt.annotation.array[x+i,y+j,z+k] != nmt.annotation.array[x,y,z]:\n",
    "                ls.append([x,y,z])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "import ast\n",
    "import time\n",
    "#ns.features['soma_features'].raw_data\n",
    "def metric_Cal(raw_data, regionINFO):\n",
    "    start = time.time()\n",
    "    raw_data = ns.features['soma_features'].raw_data\n",
    "    somaDF = raw_data.copy()\n",
    "    scaledDF = pd.DataFrame()\n",
    "    scaledDF[\"x\"] = (somaDF[\"x\"] / nmt.annotation.space[\"x\"]).copy()\n",
    "    scaledDF[\"y\"] = (somaDF[\"y\"] / nmt.annotation.space[\"y\"]).copy()\n",
    "    scaledDF[\"z\"] = (somaDF[\"z\"] / nmt.annotation.space[\"z\"]).copy()\n",
    "    #flip the somalocation\n",
    "    scaledDF[\"z\"][scaledDF['z']>(nmt.annotation.size[\"z\"]//2)] = scaledDF[\"z\"][scaledDF[\"z\"]>(nmt.annotation.size['z']//2)] -nmt.annotation.size[\"z\"]//2 \n",
    "    i_p = 0\n",
    "    for idx in scaledDF.index.tolist():\n",
    "        i_p = i_p+1\n",
    "        tempCoor = np.array([[scaledDF.loc[idx,'x'],scaledDF.loc[idx,'y'],scaledDF.loc[idx,'z']]])\n",
    "        tempID = nmt.annotation.array[int(scaledDF.loc[idx,'x']),int(scaledDF.loc[idx,'y']),int(scaledDF.loc[idx,'z'])]\n",
    "        coords = []\n",
    "        if regionINFO.loc[tempID ,'Vx']=='unknown' :\n",
    "            scaledDF.loc[idx,'SqEuclidean'] = 'unknown'\n",
    "            continue\n",
    "        if regionINFO.loc[tempID ,'Vx']=='oob' :\n",
    "            scaledDF.loc[idx,'SqEuclidean'] = 'oob'\n",
    "            continue\n",
    "        for icoor in range(len(ast.literal_eval(regionINFO.loc[tempID ,'Vx']))):\n",
    "            coords.append([ast.literal_eval(regionINFO.loc[tempID ,'Vx'])[icoor],\n",
    "                           ast.literal_eval(regionINFO.loc[tempID ,'Vy'])[icoor],\n",
    "                                            ast.literal_eval(regionINFO.loc[tempID ,'Vz'])[icoor]])\n",
    "        zs = np.sort(distance.cdist(tempCoor,coords, 'sqeuclidean'))[0,0:min(10,distance.cdist(tempCoor,coords, 'sqeuclidean').shape[1])]\n",
    "        scaledDF.loc[idx,'SqEuclidean'] = str(zs.tolist())\n",
    "        \n",
    "        print('Load progress: %.5f'% (i_p/scaledDF.shape[0]))\n",
    "    end = time.time()\n",
    "    print(\"Total loading time: %.2f\" % (end-start))\n",
    "    return scaledDF.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in scaled_result.index.tolist():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_result = metric_Cal(ns.features['soma_features'].raw_data, regionINFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_result.to_excel('/home/penglab/NMTcode/somaINFO.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for idx in scaled_result.index.tolist():\n",
    "    templist = scaled_result.loc[idx,'SqEuclidean'].replace(\"[\", \"\").replace(\"]\", \"\").replace(\" \",\"\").split(',')\n",
    "    scaled_result.loc[idx,'minDIS'] = templist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_result['minDIS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly \n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import scipy\n",
    "y = scaled_result['minDIS'].astype(float).values.tolist()\n",
    "\n",
    "fig = plotly.figure_factory.create_violin(y, title='Violin Plot', colors='#604d9e')\n",
    "#py.iplot(fig, filename='alcohol-violin-visual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots()\n",
    "\n",
    "axes.violinplot(dataset = [scaled_result['minDIS'].astype(float).values] )\n",
    "\n",
    "axes.set_title('Soma Loc v.s. Region Vertices distance')\n",
    "axes.yaxis.grid(True)\n",
    "axes.set_xlabel('')\n",
    "axes.set_ylabel('min distance')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt.annotation.array[230,13,54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(scaled_result['minDIS'].astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "have a higher density at . That is very significant because as in the SepalLengthCm description, a mean value is at 9067.528044686285."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"dark\")\n",
    "ax = sns.violinplot(x=scaled_result['minDIS'].astype(float),palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "\n",
    "# 8 points defining the cube corners\n",
    "pts = np.array(coordi)#np.array([[0, 0, 0], [1, 0, 0], [1, 1, 0], [0, 1, 0],\n",
    "                #[0, 0, 1], [1, 0, 1], [1, 1, 1], [0, 1, 1], ])\n",
    "\n",
    "hull = ConvexHull(pts)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# Plot defining corner points\n",
    "ax.plot(pts.T[0], pts.T[1], pts.T[2], \"ko\")\n",
    "\n",
    "# 12 = 2 * 6 faces are the simplices (2 simplices per square face)\n",
    "\n",
    "\n",
    "# Make axis label\n",
    "for i in [\"x\", \"y\", \"z\"]:\n",
    "    eval(\"ax.set_{:s}label('{:s}')\".format(i, i))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns.features['soma_features'].raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "import ast\n",
    "#ns.features['soma_features'].raw_data\n",
    "def metric_Cal(raw_data, regionINFO):\n",
    "    somaDF = raw_data.copy()\n",
    "    for idx in somaDF.index.tolist():\n",
    "        tempCoor = np.array([(somaDF.loc[idx,'x'],somaDF.loc[idx,'y'],somaDF.loc[idx,'z'])])\n",
    "        zs = np.sum(distance.cdist(tempCoor , ast.literal_eval(regionINFO.loc[idx,'Coor']), 'sqeuclidean'))\n",
    "        somaDF.loc[idx,'SUMsqeuclidean'] = zs\n",
    "    return somaDF.copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
